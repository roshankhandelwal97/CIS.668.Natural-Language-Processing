{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tec0Zq2VeLKw"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0gbrRjWtlk0"
      },
      "source": [
        "All imports here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WR8p7w2eJzS",
        "outputId": "06a9765c-8189-4200-f603-d8758a0a4e1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import csv\n",
        "import pandas as pd\n",
        "from nltk import FreqDist\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "import string\n",
        "from nltk.collocations import *\n",
        "import re\n",
        "\n",
        "#for tokenization\n",
        "nltk.download('punkt')\n",
        "\n",
        "#for stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk_stops = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "#for Lemmatization\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "wnl = nltk.WordNetLemmatizer()\n",
        "\n",
        "#for importing the two csvs\n",
        "from google.colab import drive\n",
        "\n",
        "# for spacy\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKXGGbdBtooC"
      },
      "source": [
        "Importing both csv's\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLB7W2V0jSH4",
        "outputId": "6c6699fa-68d9-4376-b46e-71119d47da2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount(\"/drive\", force_remount=True)\n",
        "\n",
        "fakeNewsCsv = '/drive/My Drive/Colab Notebooks/IST 664/2023/Fake.csv'\n",
        "trueNewsCsv = '/drive/My Drive/Colab Notebooks/IST 664/2023/True.csv'\n",
        "\n",
        "\n",
        "fn = pd.read_csv(fakeNewsCsv)\n",
        "tn = pd.read_csv(trueNewsCsv)\n",
        "\n",
        "fn_text = fn['text']\n",
        "tn_text = tn['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0GWgxXFRZan"
      },
      "source": [
        "Tokenization: Converting the each text article individual words or tokens, making it easier to apply the next data pre-processing steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "zirLCKYu546F"
      },
      "outputs": [],
      "source": [
        "fn_tokens = [nltk.word_tokenize(text) for text in fn_text]\n",
        "tn_tokens = [nltk.word_tokenize(text) for text in tn_text]\n",
        "\n",
        "#making a single list from a list of list of tokens\n",
        "\n",
        "fakeNewsTokens = [token for sublist in fn_tokens for token in sublist]\n",
        "trueNewsTokens = [token for sublist in tn_tokens for token in sublist]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdWcG0SMRkAO"
      },
      "source": [
        "Use lower case: Converting all tokens to lowercase ensuring uniformity and eliminating the risk of duplicate words based on case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "4F8tGwMZjADY"
      },
      "outputs": [],
      "source": [
        "fakeNewsTokens_lower = [token.lower() for token in fakeNewsTokens]\n",
        "trueNewsTokens_lower = [token.lower() for token in trueNewsTokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsXJk10XS8US"
      },
      "source": [
        "Removing all numbers or non-alphabets ensuring analysis is done on words only\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "rV86V0rOpBRk"
      },
      "outputs": [],
      "source": [
        "fakeNewsTokens_lower_aplha = [token for token in fakeNewsTokens_lower if token.isalpha()]\n",
        "trueNewsTokens_lower_aplha = [token for token in trueNewsTokens_lower if token.isalpha()]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwRyAHbjUrmQ"
      },
      "source": [
        "1. list the top 50 stop words by frequency in fake news articles\n",
        "and those in real news articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK7AfeSbTLwA",
        "outputId": "1fde7145-31f2-4bd8-a839-8ca13bea0bf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "list the top 50 stop words by frequency in fake news articles:\n",
            " [('the', 528689), ('to', 289665), ('of', 235806), ('and', 224544), ('a', 211092), ('in', 164857), ('that', 150262), ('s', 129641), ('is', 110887), ('for', 92115), ('on', 81354), ('it', 79374), ('he', 77627), ('was', 67794), ('with', 62964), ('his', 58037), ('as', 55978), ('this', 55655), ('be', 48565), ('by', 47387), ('not', 46833), ('are', 46350), ('have', 46038), ('they', 45190), ('i', 43778), ('who', 42327), ('has', 42269), ('at', 41569), ('from', 40754), ('t', 40531), ('you', 40323), ('we', 38205), ('an', 34862), ('about', 32834), ('but', 31426), ('their', 30202), ('she', 25763), ('her', 25705), ('or', 24671), ('what', 24594), ('all', 24455), ('will', 24061), ('been', 22991), ('out', 22925), ('more', 22388), ('if', 22221), ('were', 21608), ('when', 21272), ('can', 20709), ('had', 20425)]\n",
            "\n",
            "list the top 50 stop words by frequency in true news articles:\n",
            " [('the', 482176), ('to', 245125), ('of', 204959), ('a', 197860), ('and', 181686), ('in', 180673), ('on', 108346), ('s', 99902), ('that', 88116), ('for', 79730), ('is', 55439), ('with', 54485), ('he', 54368), ('it', 49364), ('was', 47940), ('by', 47637), ('as', 47307), ('has', 46243), ('from', 39376), ('his', 38016), ('not', 37594), ('have', 36415), ('be', 34295), ('at', 33164), ('an', 32710), ('who', 27642), ('but', 26596), ('are', 26127), ('had', 25672), ('will', 25241), ('we', 22163), ('they', 22064), ('its', 21319), ('this', 21025), ('which', 20809), ('been', 19600), ('after', 19375), ('were', 18916), ('about', 17425), ('more', 17197), ('i', 17020), ('their', 16826), ('or', 15107), ('over', 13841), ('some', 12752), ('if', 12538), ('she', 11721), ('there', 11642), ('other', 11581), ('than', 11299)]\n"
          ]
        }
      ],
      "source": [
        "fakeNewsTokens_Stopwords = [token for token in fakeNewsTokens_lower_aplha if token in nltk_stops]\n",
        "trueNewsTokens_Stopwords = [token for token in trueNewsTokens_lower_aplha if token in nltk_stops]\n",
        "\n",
        "fake_freqDist = FreqDist(fakeNewsTokens_Stopwords)\n",
        "true_freqDist = FreqDist(trueNewsTokens_Stopwords)\n",
        "\n",
        "print(\"list the top 50 stop words by frequency in fake news articles:\\n\",fake_freqDist.most_common(50))\n",
        "print(\"\\nlist the top 50 stop words by frequency in true news articles:\\n\",true_freqDist.most_common(50))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yJxMsdFTDIM"
      },
      "source": [
        "Removing all stop words from the list\n",
        "\n",
        "2. list the top 50 content words by frequency in fake news articles and those in real news articles\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rK9odtfalzAX",
        "outputId": "e8097d6c-b112-4233-b20d-22cb72829b08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "list the top 50 content words by frequency in fake news articles:\n",
            " [('trump', 74241), ('said', 31149), ('people', 26015), ('president', 25772), ('would', 23461), ('one', 22995), ('clinton', 18085), ('obama', 17920), ('like', 17660), ('donald', 17235), ('also', 15243), ('new', 14198), ('news', 14198), ('us', 13922), ('even', 13691), ('hillary', 13673), ('white', 12797), ('time', 12792), ('state', 12543), ('via', 11355), ('media', 11061), ('get', 10710), ('america', 10648), ('house', 10604), ('campaign', 10563), ('know', 10287), ('could', 10230), ('first', 10032), ('american', 9949), ('going', 9746), ('many', 9719), ('image', 9630), ('states', 9529), ('make', 9152), ('told', 9103), ('republican', 9009), ('right', 8902), ('country', 8687), ('made', 8667), ('government', 8603), ('police', 8576), ('say', 8554), ('way', 8464), ('back', 8407), ('think', 8358), ('two', 8343), ('years', 8264), ('video', 8145), ('election', 8023), ('united', 7978)]\n",
            "\n",
            "list the top 50 content words by frequency in true news articles: \n",
            " [('said', 99037), ('trump', 54249), ('would', 31526), ('reuters', 28412), ('president', 26397), ('state', 19728), ('government', 18288), ('new', 16784), ('house', 16519), ('states', 16515), ('also', 15946), ('united', 15576), ('republican', 15346), ('people', 15138), ('told', 14244), ('could', 13709), ('one', 12676), ('last', 12613), ('party', 12436), ('washington', 12419), ('two', 11620), ('election', 11482), ('year', 10972), ('former', 10601), ('campaign', 10561), ('donald', 10447), ('security', 10079), ('percent', 9937), ('north', 9870), ('clinton', 9457), ('white', 9443), ('court', 9406), ('senate', 9206), ('obama', 9197), ('country', 8700), ('minister', 8660), ('china', 8563), ('first', 8549), ('officials', 8474), ('since', 8332), ('tuesday', 8263), ('democratic', 8237), ('week', 8226), ('foreign', 8197), ('administration', 8194), ('national', 8184), ('including', 8119), ('presidential', 8011), ('wednesday', 8009), ('military', 7996)]\n"
          ]
        }
      ],
      "source": [
        "fakeNewsTokens_noStopwords = [token for token in fakeNewsTokens_lower_aplha if token not in nltk_stops]\n",
        "trueNewsTokens_noStopwords = [token for token in trueNewsTokens_lower_aplha if token not in nltk_stops]\n",
        "\n",
        "fake_freqDist = FreqDist(fakeNewsTokens_noStopwords)\n",
        "true_freqDist = FreqDist(trueNewsTokens_noStopwords)\n",
        "\n",
        "print(\"list the top 50 content words by frequency in fake news articles:\\n\",fake_freqDist.most_common(50))\n",
        "print(\"\\nlist the top 50 content words by frequency in true news articles: \\n\",true_freqDist.most_common(50))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQYY--g-3lAJ"
      },
      "source": [
        "3. list the top 50 bigrams by frequencies in fake news articles and those in real news articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knYqkeC3119m",
        "outputId": "72743118-986e-4624-cf0d-abeb5e0a6be7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "list the top 50 bigrams by frequencies in fake news articles:\n",
            " [(('donald', 'trump'), 15354), (('hillary', 'clinton'), 6783), (('white', 'house'), 6345), (('image', 'via'), 6223), (('united', 'states'), 6198), (('new', 'york'), 4207), (('president', 'obama'), 3851), (('president', 'trump'), 3761), (('fox', 'news'), 3243), (('barack', 'obama'), 2240), (('featured', 'image'), 2088), (('century', 'wire'), 1926), (('fake', 'news'), 1774), (('supreme', 'court'), 1772), (('trump', 'campaign'), 1708), (('trump', 'realdonaldtrump'), 1701), (('obama', 'administration'), 1666), (('national', 'security'), 1651), (('social', 'media'), 1642), (('law', 'enforcement'), 1566), (('secretary', 'state'), 1542), (('york', 'times'), 1513), (('washington', 'post'), 1512), (('state', 'department'), 1499), (('republican', 'party'), 1451), (('american', 'people'), 1436), (('attorney', 'general'), 1414), (('lives', 'matter'), 1394), (('bernie', 'sanders'), 1390), (('trump', 'said'), 1373), (('ted', 'cruz'), 1347), (('trump', 'supporters'), 1339), (('black', 'lives'), 1337), (('screen', 'capture'), 1323), (('bill', 'clinton'), 1302), (('mainstream', 'media'), 1266), (('last', 'week'), 1246), (('last', 'year'), 1245), (('even', 'though'), 1241), (('trump', 'administration'), 1193), (('president', 'donald'), 1033), (('president', 'united'), 1010), (('presidential', 'candidate'), 1006), (('climate', 'change'), 1000), (('police', 'officers'), 991), (('breitbart', 'news'), 957), (('getty', 'images'), 956), (('news', 'century'), 943), (('president', 'barack'), 922), (('years', 'ago'), 911)]\n",
            "\n",
            "list the top 50 bigrams by frequencies in true news articles:\n",
            " [(('united', 'states'), 12086), (('donald', 'trump'), 10146), (('white', 'house'), 8372), (('washington', 'reuters'), 6497), (('president', 'donald'), 5926), (('north', 'korea'), 5629), (('new', 'york'), 4450), (('prime', 'minister'), 4132), (('said', 'statement'), 3938), (('trump', 'said'), 3498), (('told', 'reuters'), 3492), (('islamic', 'state'), 3384), (('barack', 'obama'), 3342), (('last', 'year'), 3330), (('last', 'week'), 3305), (('told', 'reporters'), 3176), (('said', 'would'), 3022), (('president', 'barack'), 2904), (('hillary', 'clinton'), 2483), (('supreme', 'court'), 2466), (('last', 'month'), 2463), (('trump', 'administration'), 2443), (('secretary', 'state'), 2264), (('united', 'nations'), 2264), (('house', 'representatives'), 2257), (('said', 'trump'), 2146), (('national', 'security'), 2119), (('human', 'rights'), 2068), (('reuters', 'president'), 2050), (('official', 'said'), 1990), (('european', 'union'), 1934), (('saudi', 'arabia'), 1902), (('presidential', 'election'), 1874), (('state', 'department'), 1860), (('said', 'wednesday'), 1845), (('said', 'thursday'), 1685), (('foreign', 'minister'), 1630), (('said', 'tuesday'), 1604), (('officials', 'said'), 1586), (('trump', 'campaign'), 1560), (('said', 'friday'), 1546), (('attorney', 'general'), 1471), (('republican', 'presidential'), 1427), (('said', 'monday'), 1413), (('news', 'conference'), 1408), (('south', 'korea'), 1408), (('obama', 'administration'), 1329), (('north', 'korean'), 1272), (('also', 'said'), 1268), (('next', 'year'), 1233)]\n"
          ]
        }
      ],
      "source": [
        "fake_bigrams = FreqDist(list(ngrams(fakeNewsTokens_noStopwords, 2)))\n",
        "true_bigrams = FreqDist(list(ngrams(trueNewsTokens_noStopwords, 2)))\n",
        "\n",
        "\n",
        "print(\"list the top 50 bigrams by frequencies in fake news articles:\\n\",fake_bigrams.most_common(50))\n",
        "print(\"\\nlist the top 50 bigrams by frequencies in true news articles:\\n\",true_bigrams.most_common(50))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVFBSZDFAnPX"
      },
      "source": [
        "4. list the top 50 bigrams by their Mutual Information scores (using min frequency 5) in fake news\n",
        "articles and those in real news articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnrbpLXg9HJk",
        "outputId": "516b57fd-2d60-40f9-8805-ec086cf58736"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "list the top 50 bigrams by their Mutual Information scores (using min frequency 5) in fake news articles:\n",
            "\n",
            "(('//t.co/ltdtbehhgh', 'pic.twitter.com/t2s8ufif5o'), 21.07391482146583)\n",
            "(('0000', '0907'), 21.07391482146583)\n",
            "(('0907', '84b4'), 21.07391482146583)\n",
            "(('314a', '3453'), 21.07391482146583)\n",
            "(('3453', '0000'), 21.07391482146583)\n",
            "(('4d6c', '6330'), 21.07391482146583)\n",
            "(('5707', '5736'), 21.07391482146583)\n",
            "(('5774', '6a7a'), 21.07391482146583)\n",
            "(('6330', '666b'), 21.07391482146583)\n",
            "(('666b', '314a'), 21.07391482146583)\n",
            "(('6a7a', '4d6c'), 21.07391482146583)\n",
            "(('7616', '86f7'), 21.07391482146583)\n",
            "(('84b4', 'f787'), 21.07391482146583)\n",
            "(('86f7', 'a737'), 21.07391482146583)\n",
            "(('a737', '5707'), 21.07391482146583)\n",
            "(('acab', 'pic.twitter.com/naqnehnd5g'), 21.07391482146583)\n",
            "(('f787', '7616'), 21.07391482146583)\n",
            "(('kambree', 'kawahine'), 21.07391482146583)\n",
            "(('kawahine', 'koa'), 21.07391482146583)\n",
            "(('lynnette', 'hardway'), 21.07391482146583)\n",
            "(('managementvideo', 'solutionsvideo'), 21.07391482146583)\n",
            "(('myocardial', 'infarction'), 21.07391482146583)\n",
            "(('palos', 'verdes'), 21.07391482146583)\n",
            "(('pic.twitter.com/pxbrcgypwm', \"'gitmo\"), 21.07391482146583)\n",
            "(('platformvideo', 'managementvideo'), 21.07391482146583)\n",
            "(('r.t.', 'rybak'), 21.07391482146583)\n",
            "(('today.4767', '5774'), 21.07391482146583)\n",
            "(('vis-', '-vis'), 21.07391482146583)\n",
            "(('300m', 'employee-related'), 20.810880415632038)\n",
            "(('bacha', 'bazi'), 20.810880415632038)\n",
            "(('bucolic', 'adirondacks'), 20.810880415632038)\n",
            "(('fern', 'ndez'), 20.810880415632038)\n",
            "(('kel', 'inen'), 20.810880415632038)\n",
            "(('miyoshi', 'jager'), 20.810880415632038)\n",
            "(('mondaiale', 'commerciale'), 20.810880415632038)\n",
            "(('psi', 'upsilon'), 20.810880415632038)\n",
            "(('sedrick', 'tydus'), 20.810880415632038)\n",
            "(('kevork', 'djansezian/getty'), 20.810880415632035)\n",
            "(('paolo', 'gentiloni'), 20.810880415632035)\n",
            "(('semper', 'fi'), 20.810880415632035)\n",
            "(('bendavid', 'grabinski'), 20.58848799429559)\n",
            "(('dani', 'bostick'), 20.58848799429559)\n",
            "(('pic.twitter.com/fs297jooqi', 'vivelafrance'), 20.58848799429559)\n",
            "(('roller', 'coaster'), 20.58848799429559)\n",
            "(('svenska', 'dagbladet'), 20.58848799429559)\n",
            "(('yik', 'yak'), 20.58848799429559)\n",
            "(('283', '-138'), 20.588487994295587)\n",
            "(('g.k.', 'butterfield'), 20.588487994295587)\n",
            "(('neilson', 'barnard/getty'), 20.588487994295587)\n",
            "(('ria', 'novosti'), 20.588487994295587)\n",
            "\n",
            "list the top 50 bigrams by their Mutual Information scores (using min frequency 5) in fake news articles:\n",
            "\n",
            "(('agua', 'bonita'), 20.830254141762293)\n",
            "(('clarece', 'polke'), 20.830254141762293)\n",
            "(('darz', 'aab'), 20.830254141762293)\n",
            "(('dori', 'esfahani'), 20.830254141762293)\n",
            "(('doxycycline', 'hyclate'), 20.830254141762293)\n",
            "(('ejaz', 'ashrafi'), 20.830254141762293)\n",
            "(('ettore', 'rosato'), 20.830254141762293)\n",
            "(('jabha', 'shamiya'), 20.830254141762293)\n",
            "(('kajo', 'keji'), 20.830254141762293)\n",
            "(('lista', 'marjana'), 20.830254141762293)\n",
            "(('maale', 'adumim'), 20.830254141762293)\n",
            "(('marjana', 'sarca'), 20.830254141762293)\n",
            "(('mohseni', 'ejei'), 20.830254141762293)\n",
            "(('petroleo', 'brasileiro'), 20.830254141762293)\n",
            "(('aqeel', 'al-tayyar'), 20.567219735928497)\n",
            "(('ballard', 'spahr'), 20.567219735928497)\n",
            "(('beji', 'caid'), 20.567219735928497)\n",
            "(('caid', 'essebsi'), 20.567219735928497)\n",
            "(('gudni', 'johannesson'), 20.567219735928497)\n",
            "(('hammam', 'al-alil'), 20.567219735928497)\n",
            "(('irina', 'bokova'), 20.567219735928497)\n",
            "(('jabar', 'al-luaibi'), 20.567219735928497)\n",
            "(('jakobsd', 'ttir'), 20.567219735928497)\n",
            "(('malala', 'yousafzai'), 20.567219735928497)\n",
            "(('marielena', 'hincapie'), 20.567219735928497)\n",
            "(('mato', 'grosso'), 20.567219735928497)\n",
            "(('mens', 'rea'), 20.567219735928497)\n",
            "(('moqtada', 'al-sadr'), 20.567219735928497)\n",
            "(('murithi', 'mutiga'), 20.567219735928497)\n",
            "(('najia', 'bounaim'), 20.567219735928497)\n",
            "(('ngoc', 'nhu'), 20.567219735928497)\n",
            "(('sinan', 'ulgen'), 20.567219735928497)\n",
            "(('sobhan', 'chowdhury'), 20.567219735928497)\n",
            "(('lashkar', 'gah'), 20.344827314592052)\n",
            "(('palong', 'khali'), 20.344827314592052)\n",
            "(('satya', 'nadella'), 20.344827314592052)\n",
            "(('triacetone', 'triperoxide'), 20.344827314592052)\n",
            "(('verisk', 'maplecroft'), 20.344827314592052)\n",
            "(('ant', 'nio'), 20.34482731459205)\n",
            "(('arkady', 'dvorkovich'), 20.34482731459205)\n",
            "(('bucky', 'hellwig'), 20.34482731459205)\n",
            "(('gwede', 'mantashe'), 20.34482731459205)\n",
            "(('hemin', 'hawrami'), 20.34482731459205)\n",
            "(('kembo', 'mohadi'), 20.34482731459205)\n",
            "(('linas', 'linkevicius'), 20.34482731459205)\n",
            "(('maithripala', 'sirisena'), 20.34482731459205)\n",
            "(('mata', 'pires'), 20.34482731459205)\n",
            "(('myong', 'guk'), 20.34482731459205)\n",
            "(('nickolay', 'mladenov'), 20.34482731459205)\n",
            "(('purwo', 'nugroho'), 20.34482731459205)\n"
          ]
        }
      ],
      "source": [
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "finder = BigramCollocationFinder.from_words(fakeNewsTokens_lower)\n",
        "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
        "\n",
        "def alpha_filter(w):\n",
        "  # pattern to non-aplha characters\n",
        "  pattern = re.compile('^[^a-z]+$')\n",
        "  if (pattern.match(w)):\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "finder4 = BigramCollocationFinder.from_words(fakeNewsTokens_lower)\n",
        "finder4.apply_freq_filter(5)\n",
        "scored = finder4.score_ngrams(bigram_measures.pmi)\n",
        "print(\"\\nlist the top 50 bigrams by their Mutual Information scores (using min frequency 5) in fake news articles:\\n\")\n",
        "for bscore_1 in scored[:50]:\n",
        "    print (bscore_1)\n",
        "\n",
        "finder4 = BigramCollocationFinder.from_words(trueNewsTokens_lower)\n",
        "finder4.apply_freq_filter(5)\n",
        "scored = finder4.score_ngrams(bigram_measures.pmi)\n",
        "print(\"\\nlist the top 50 bigrams by their Mutual Information scores (using min frequency 5) in fake news articles:\\n\")\n",
        "for bscore_2 in scored[:50]:\n",
        "    print (bscore_2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization: bringing word to it's base form. This step ensures that different forms of a word (\"jump\" and \"jumping\") are treated as the same word"
      ],
      "metadata": {
        "id": "BA5a7dSkUohs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qWVV1vKHued",
        "outputId": "a31ca736-d4d8-420c-95ae-1cfc9ed7b71c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "list the top 50 words  after lemmatization in fake news article\n",
            " [('trump', 74403), ('said', 31149), ('president', 26340), ('people', 26098), ('one', 23812), ('would', 23461), ('state', 22072), ('clinton', 18717), ('like', 18207), ('obama', 17920), ('time', 17885), ('donald', 17235), ('american', 16093), ('republican', 16061), ('say', 15528), ('also', 15243), ('year', 14843), ('new', 14198), ('news', 14198), ('u', 14181), ('image', 13937), ('even', 13692), ('hillary', 13678), ('white', 13146), ('right', 12698), ('get', 12230), ('know', 11947), ('make', 11534), ('via', 11355), ('woman', 11207), ('medium', 11142), ('campaign', 11069), ('house', 10774), ('country', 10770), ('america', 10703), ('could', 10230), ('first', 10041), ('want', 9818), ('think', 9765), ('going', 9750), ('many', 9719), ('way', 9394), ('election', 9297), ('day', 9217), ('told', 9103), ('government', 9079), ('thing', 8962), ('video', 8903), ('made', 8667), ('back', 8611)]\n",
            "\n",
            "list the top 50 words after lemmatization in true news article\n",
            " [('said', 99037), ('trump', 54280), ('state', 36243), ('would', 31526), ('reuters', 28412), ('president', 26939), ('republican', 22098), ('government', 19433), ('year', 18721), ('house', 16906), ('new', 16784), ('also', 15946), ('united', 15576), ('people', 15207), ('party', 14964), ('official', 14575), ('told', 14244), ('country', 13948), ('election', 13910), ('could', 13709), ('one', 13025), ('last', 12631), ('washington', 12419), ('two', 11620), ('group', 11104), ('campaign', 11079), ('former', 10601), ('leader', 10504), ('donald', 10447), ('week', 10426), ('security', 10382), ('court', 10338), ('percent', 9937), ('say', 9934), ('north', 9870), ('minister', 9541), ('clinton', 9504), ('white', 9500), ('tax', 9230), ('law', 9221), ('senate', 9206), ('obama', 9197), ('time', 9048), ('vote', 8981), ('month', 8754), ('china', 8564), ('first', 8550), ('national', 8534), ('statement', 8522), ('administration', 8378)]\n"
          ]
        }
      ],
      "source": [
        "fakeLemma = [wnl.lemmatize(token) for token in fakeNewsTokens_noStopwords]\n",
        "trueLemma = [wnl.lemmatize(token) for token in trueNewsTokens_noStopwords]\n",
        "\n",
        "fakeLemmaList = FreqDist(list(fakeLemma))\n",
        "trueLemmaList = FreqDist(list(trueLemma))\n",
        "\n",
        "print(\"\\nlist the top 50 words  after lemmatization in fake news article\\n\", fakeLemmaList.most_common(50))\n",
        "print(\"\\nlist the top 50 words after lemmatization in true news article\\n\", trueLemmaList.most_common(50))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ouDFV-RI7XQ",
        "outputId": "71010625-c6a6-42ea-b5ad-850bcdd5fc38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 50 Adjective Words in Fake News Titles:\n",
            "['more', 'other', 'many', 'american', 'last', 'new', 'political', 'former', 'first', 'own', 'republican', 'same', 'black', 'presidential', 'white', 'good', 'federal', 'right', 'public', 'old', 'russian', 'anti', 'such', 'great', 'national', 'real', 'most', 'little', 'few', 'democratic', 'clear', 'big', 'much', 'several', 'illegal', 'social', 'free', 'only', 'foreign', 'recent', 'high', 'conservative', 'next', 'sure', 'full', 'bad', 'muslim', 'entire', 'able', 'featured']\n",
            "\n",
            "Top 50 Adjective Words in Real News Titles:\n",
            "['last', 'more', 'other', 'republican', 'former', 'new', 'presidential', 'political', 'first', 'many', 'democratic', 'military', 'russian', 'foreign', 'such', 'federal', 'nuclear', 'next', 'public', 'national', 'top', 'senior', 'economic', 'several', 'international', 'american', 'major', 'recent', 'old', 'own', 'legal', 'conservative', 'possible', 'financial', 'local', 'anti', 'clear', 'same', 'syrian', 'most', 'high', 'chinese', 'long', 'likely', 'korean', 'good', 'private', 'congressional', 'current', 'important']\n"
          ]
        }
      ],
      "source": [
        "def adjectives(text):\n",
        "    doc = nlp(text)\n",
        "    adjectives = [token.text.lower() for token in doc if token.pos_ == \"ADJ\"]\n",
        "    return adjectives\n",
        "\n",
        "#calling the adjectives finction\n",
        "fn_adj_list = [adjectives(text) for text in fn_text]\n",
        "tn_adj_list = [adjectives(text) for text in tn_text]\n",
        "\n",
        "\n",
        "#Converting into a single list\n",
        "fn_adj = [adj for sublist in fn_adj_list for adj in sublist]\n",
        "tn_adj = [adj for sublist in tn_adj_list for adj in sublist]\n",
        "\n",
        "fn_adj_alpha = [token for token in fn_adj if token.isalpha()]\n",
        "tn_adj_alpha = [token for token in tn_adj if token.isalpha()]\n",
        "\n",
        "fn_adjective_freq = Counter(fn_adj_alpha)\n",
        "tn_adjective_freq = Counter(tn_adj_alpha)\n",
        "\n",
        "fn_adjectives_common = [word for word, freq in fn_adjective_freq.most_common(50)]\n",
        "tn_adjectives_common = [word for word, freq in tn_adjective_freq.most_common(50)]\n",
        "\n",
        "# Print the top 50 adjective words for fake and real news titles\n",
        "print(\"Top 50 Adjective Words in Fake News Titles:\")\n",
        "print(fn_adjectives_common)\n",
        "\n",
        "print(\"\\nTop 50 Adjective Words in Real News Titles:\")\n",
        "print(tn_adjectives_common)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate word count\n",
        "def wordCount(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return len(tokens)\n",
        "\n",
        "# Function to calculate content word count\n",
        "def contentWordCount(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    content_tokens = [token for token in tokens if token.lower() not in nltk_stops and token not in string.punctuation]\n",
        "    return len(content_tokens)\n",
        "\n",
        "# Function to calculate uppercase word count (excluding \"I\")\n",
        "def uppercaseWordCount(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    uppercase_tokens = [token for token in tokens if token.isupper() and token != \"I\"]\n",
        "    return len(uppercase_tokens)\n",
        "\n",
        "# Function to calculate exclamation mark count\n",
        "def exclamationMarkCount(text):\n",
        "    return text.count(\"!\")\n",
        "\n",
        "# Function to calculate punctuation mark count\n",
        "def punctuationMarkCount(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    count = sum(1 for token in tokens if token in string.punctuation)\n",
        "    return count\n",
        "\n",
        "# Calculate and add new columns to the DataFrames\n",
        "fn[\"WordCount\"] = fn[\"text\"].apply(wordCount)\n",
        "fn[\"ContentWordCount\"] = fn[\"text\"].apply(contentWordCount)\n",
        "fn[\"UppercaseWordCount\"] = fn[\"text\"].apply(uppercaseWordCount)\n",
        "fn[\"ExclamationMarkCount\"] = fn[\"text\"].apply(exclamationMarkCount)\n",
        "fn[\"PunctuationMarkCount\"] = fn[\"text\"].apply(punctuationMarkCount)\n",
        "\n",
        "tn[\"WordCount\"] = tn[\"text\"].apply(wordCount)\n",
        "tn[\"ContentWordCount\"] = tn[\"text\"].apply(contentWordCount)\n",
        "tn[\"UppercaseWordCount\"] = tn[\"text\"].apply(uppercaseWordCount)\n",
        "tn[\"ExclamationMarkCount\"] = tn[\"text\"].apply(exclamationMarkCount)\n",
        "tn[\"PunctuationMarkCount\"] = tn[\"text\"].apply(punctuationMarkCount)\n",
        "\n",
        "# Save the updated DataFrames back to CSV files\n",
        "fn.to_csv(\"fake_news_with_counts.csv\", index=False)\n",
        "tn.to_csv(\"true_news_with_counts.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "_5H9vqSOsPk9"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result Analysis::\n",
        "\n",
        "\n",
        "\n",
        "*   On an average, about 4 exclamation were used in fake news arctile, comapred to 0.006 exclamation in true news arctile.\n",
        "\n",
        "\n",
        "*   though both fake news and real news had political references, it is seen that content words in fake news article show more policital reference.\n",
        "\n",
        "\n",
        "\n",
        "*   The result of bigrams by their Mutual Information scores shows that fake news arctile indicate the presence of gibberish or non-english words . Whereas real news article represents much meaningful words and pairs.\n",
        "\n",
        "\n",
        "*   On analysisng the adjectives in the two article, we see that quatitavive words (many, most, little, little, big, much, only, full) occur much more in fake news article than real news.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fms0FgmkOTdJ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}