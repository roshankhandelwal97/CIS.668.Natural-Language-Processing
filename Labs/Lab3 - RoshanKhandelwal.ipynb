{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRWAxlXmZ3qG"
      },
      "source": [
        "**IST664: Week 3 Lab**\n",
        "\n",
        "In the realm of natural language processing, the term \"morphology\" refers to the study of words and how they are formed. In previous labs we have seen that even the seemingly simple task of tokenization has many complications. In this lab, we take a deeper dive into the world of words by examining how stemmers work, some of the complexities of part of speech tagging, and how we can represent words as high dimensional vectors.\n",
        "\n",
        "Although contemporary deep learning methods tend to hide a lot of these details behind the veil of the neural network, it is still critically important to have an essential understanding of morphology and how it can impact higher levels of representation. Your ability to create, debug, and successfully modify a natural language system will be enhanced by deepening your understanding of how we use code to assign meaning to various parts of speech.\n",
        "\n",
        "This lab begins by reading a complete text from the Project Gutenberg website. We are downloading Dostoevsky's Crime and Punishment, as plain text, in a translation by Constance Garnett."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9wxZxwNb3dN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8c5e14a-f32a-47a9-d211-0a100d2f70b4"
      },
      "source": [
        "import nltk # We'll be using lots of facilities from this\n",
        "nltk.download('punkt') # Download, as not included in basic colab\n",
        "\n",
        "# text from online gutenberg\n",
        "from urllib import request # We will need this to read from the URL\n",
        "\n",
        "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf8')\n",
        "type(raw), len(raw)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(str, 1176812)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw0XuitLcSiR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "3befb19e-a435-4d88-97d2-564e3dd1319b"
      },
      "source": [
        "# Over one million characters. Let's look at the first few.\n",
        "raw[:178]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ufeffThe Project Gutenberg eBook of Crime and Punishment, by Fyodor Dostoevsky\\r\\n\\r\\nThis eBook is for the use of anyone anywhere in the United States and\\r\\nmost other parts of the world'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVfL-VuX9rNH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c26716de-0118-468d-ad4b-5f04b8c29fe1"
      },
      "source": [
        "# We'll begin our processing with tokenization using punkt\n",
        "crimetokens = nltk.word_tokenize(raw)\n",
        "crimetokens[112:122]\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Release', 'Date', ':', 'March', ',', '2001', '[', 'eBook', '#', '2554']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVVkw7kFfIYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d474e26-f86a-42ca-a20a-570eb22575f9"
      },
      "source": [
        "# Let's keep track of how many unique tokens we're starting with.\n",
        "len(set(crimetokens))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11516"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Sb09Wv-ekV_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1973797e-e659-4547-fb9a-ee3c58473f61"
      },
      "source": [
        "crimetokens = [w.lower() for w in crimetokens]\n",
        "crimetokens[112:122]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['release', 'date', ':', 'march', ',', '2001', '[', 'ebook', '#', '2554']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptGA8TN4erc4"
      },
      "source": [
        "**Part 1 - Stemming and Lemmatization**\n",
        "\n",
        "Our first task will be to examine stemming - the process of removing endings from words. Stemming can be considered as a \"data reduction\" method that may be helpful for simplifying downstream analysis. For example, tokens like eat, eats, eaten, and eating will all be stemmed to eat. As with tokenization, there are many approaches to stemming and each yields somewhat different results. Stemming is also sometimes referred to as a \"normalization\" technique. Another, even simpler normalization technique we saw earlier in the course is lowercasing. One goal of all normalization techniques is to reduce the overall size of the vocaubulary, with the intent of improving the performance of later tasks such as searching. We will compare three stemmers provided by NLTK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSK7UQRn9rNQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cda5277-ea8c-4a6d-d468-6de6fe1da8bf"
      },
      "source": [
        "porter = nltk.PorterStemmer()\n",
        "lancaster = nltk.LancasterStemmer()\n",
        "snowball = nltk.stem.SnowballStemmer('english')\n",
        "type(porter), type(lancaster), type(snowball)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(nltk.stem.porter.PorterStemmer,\n",
              " nltk.stem.lancaster.LancasterStemmer,\n",
              " nltk.stem.snowball.SnowballStemmer)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb-Lu1I0h34S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2561139a-6495-47b3-9597-fc90cbec7ea9"
      },
      "source": [
        "# Let's test each one\n",
        "porter.stem('eatery')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eateri'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nR9-wu0sh-7q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7373633b-4166-4453-df96-b8f19400af4c"
      },
      "source": [
        "lancaster.stem('eatery')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eatery'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0UREP3jiCYb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c13bd1bd-ec9b-4bb3-d93d-e27023ad5855"
      },
      "source": [
        "snowball.stem('eatery')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eateri'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHE3VpwwiXhU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a58ce1a-e339-4852-81cf-9452f242ffea"
      },
      "source": [
        "# What happened with the examples above. Can you think of another word\n",
        "# that might be stemmed inconsistently by these stemmers? Add your word\n",
        "# in a call to these stemmers:\n",
        "\n",
        "# 3.1: Use the Porter stemmer to stem a new word\n",
        "print(porter.stem('happiness') )\n",
        "\n",
        "\n",
        "# 3.2: Use the Lancaster stemmer to stem a new word\n",
        "print(lancaster.stem('happiness'))\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "happi\n",
            "happy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_iPNGkai58S"
      },
      "source": [
        "Computer scientist Martin Porter wrote and published the Porter Stemmer more than 40 years ago. The Porter stemmer is a rule-based algorithm (i.e., no dictionary) for \"suffix stripping.\" The algorithm was subsequently implemented by other coders in more than two dozen different computer languages. Eventually, Porter got tired of hearing about the implementation errors in some of these other versions, so he rewrote the algorithm in C about 20 years ago. He also created a programming framework, called \"Snowball\" that can be used to create additional stemmers including the third one above, which is also known as the Porter2 stemmer. You can see the latest state of the snowball language and rule based stemmers here: https://snowballstem.org\n",
        "\n",
        "The Lancaster stemmer, also known as the Paice/Husk stemmer, was created at Lancaster University and has the advantage that the \"rule book\" it uses is external to the algorithm itself and can therefore be adapted to languages other than English."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMgJIRW-eMVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "792142d0-0ca2-44f2-f194-9663a2408adf"
      },
      "source": [
        "# From a data reduction standpoint, which stemmer results in the greatest\n",
        "# reduction in the number of unique tokens? Remember that we started with 11539.\n",
        "crimePstem = [porter.stem(t) for t in crimetokens]\n",
        "crimeLstem = [lancaster.stem(t) for t in crimetokens]\n",
        "crimeSstem = [snowball.stem(t) for t in crimetokens]\n",
        "\n",
        "len(set(crimePstem)), len(set(crimeLstem)), len(set(crimeSstem))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7363, 6399, 7174)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mgq8DbXpPE8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a30ac219-d74a-4c7b-e9ea-9e57fb259426"
      },
      "source": [
        "# What percentage reduction have we achieved with the Porter stemmer?\n",
        "round( 100 - (len(set(crimePstem))/len(set(crimetokens)) * 100), 1)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31.1"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv2hXaupp-Ia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b707386-5f1f-4487-bbe1-c01e5fc97144"
      },
      "source": [
        "# The Porter stemmer was the least aggressive of the three. Now calculate and\n",
        "# show the percent reduction in the number of tokens for the MOST aggressive\n",
        "# of the three stemmers.\n",
        "\n",
        "# 3.3: Compute and display percent reduction\n",
        "round( 100 - (len(set(crimeLstem))/len(set(crimetokens)) * 100), 1)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40.1"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRdy05ubPszU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76117711-aabf-44c7-d6df-49b13696221d"
      },
      "source": [
        "# Let's compare the highest frequency tokens from the three stemmers\n",
        "from nltk import FreqDist\n",
        "pdist = FreqDist(crimePstem)\n",
        "ldist = FreqDist(crimeLstem)\n",
        "sdist = FreqDist(crimeSstem)\n",
        "\n",
        "# zip() is a cool built-in function for zipping together two or\n",
        "# more lists/tuples into a single iterator.\n",
        "compare = zip(pdist.most_common(20),\n",
        "              ldist.most_common(20),\n",
        "              sdist.most_common(20))\n",
        "print(\" - Porter -    - Lancaster -   - Snowball -\") # Make a heading\n",
        "[i for i in compare] # Here we use the iterator to show the 3 sets of results"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " - Porter -    - Lancaster -   - Snowball -\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((',', 16177), (',', 16177), (',', 16177)),\n",
              " (('.', 8908), ('.', 8908), ('.', 8908)),\n",
              " (('the', 8006), ('the', 8038), ('the', 8006)),\n",
              " (('and', 7031), ('and', 7031), ('and', 7031)),\n",
              " (('to', 5350), ('to', 5350), ('to', 5350)),\n",
              " (('he', 4769), ('he', 4769), ('he', 4769)),\n",
              " (('a', 4651), ('a', 4651), ('a', 4651)),\n",
              " (('i', 4397), ('i', 4397), ('i', 4397)),\n",
              " (('you', 4086), ('you', 4094), ('you', 4086)),\n",
              " (('’', 4039), ('’', 4039), ('’', 4039)),\n",
              " (('“', 3980), ('“', 3980), ('“', 3980)),\n",
              " (('”', 3929), ('”', 3929), ('”', 3929)),\n",
              " (('of', 3927), ('of', 3927), ('of', 3927)),\n",
              " (('it', 3474), ('it', 3474), ('it', 3474)),\n",
              " (('that', 3282), ('that', 3282), ('that', 3282)),\n",
              " (('in', 3248), ('in', 3261), ('in', 3248)),\n",
              " (('wa', 2826), ('was', 2826), ('was', 2826)),\n",
              " (('!', 2364), ('on', 2606), ('!', 2364)),\n",
              " (('?', 2275), ('!', 2364), ('?', 2275)),\n",
              " (('hi', 2114), ('?', 2275), ('his', 2113))]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7hFZwVJRbYr"
      },
      "source": [
        "There's a lot going on in the display just above. All three stemmers agree on commas, periods, the word \"and,\" and the close double quote. Can you think of some hypotheses for why the word \"the\" has a different count for the Lancaster stemmer?\n",
        "\n",
        "What's going on near the end of the list where we have the following output:\n",
        "\n",
        "(('wa', 2825), ('was', 2825), ('was', 2825))\n",
        "\n",
        "The counts match, but what has the Porter stemmer done differently? Even based on the small amount of evidence above, what conclusions can you draw about the advantages and disadvantages of stemmers?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2P932lMTYcJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ad50e0a-f9e3-471d-df26-aadf745a0d8e"
      },
      "source": [
        "# Now, as an exercise, compare the hapaxes from the three FreqDist objects.\n",
        "# Try to create a nice compact display that highlights some of the similarities\n",
        "# and differences between the stemmers.\n",
        "\n",
        "# 3.4: Compare lists of hapaxes between the three stemmers\n",
        "\n",
        "p_hapaxes = pdist.hapaxes()\n",
        "l_hapaxes = ldist.hapaxes()\n",
        "s_hapaxes = sdist.hapaxes()\n",
        "\n",
        "print(p_hapaxes[:20])\n",
        "print(l_hapaxes[:20])\n",
        "print(s_hapaxes[:20])\n",
        "# For reminders on how to access hapaxes, check the Week 2 lab."
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeffthe', '#', '2554', 'august', '6', '2021', 'encod', 'utf-8', 'bicker', 'dagni', 'david', 'widger', 'prefac', 'reader', 'hard-work', 'engin', 'folk.', 'nekrassov', 'review', 'acclam']\n",
            "['\\ufeffthe', '#', '2554', 'august', '6', '2021', 'encod', 'utf-8', 'bick', 'dagny', 'david', 'widg', 'prefac', 'hard-working', 'engin', 'folk.', 'nekrassov', 'review', 'acclam', '1849']\n",
            "['\\ufeffthe', '#', '2554', 'august', '6', '2021', 'encod', 'utf-8', 'bicker', 'dagni', 'david', 'widger', 'prefac', 'reader', 'hard-work', 'engin', 'folk.', 'nekrassov', 'review', 'acclam']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vahxE6EaT96Y"
      },
      "source": [
        "A lemma is the root form on a word. In English one of the most striking set of lemmas comes from the verb \"to be.\" The words \"am,\" \"is,\" \"are,\" and \"be,\" despite their unique spellings and pronunciations, all lemmatize to \"be.\" Let's try this with the Wordnet Lemmatizer:  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjBF4aidUt0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6a036ad-a799-4311-fae1-b93c05f47f37"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "wnl = nltk.WordNetLemmatizer()\n",
        "type(wnl)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nltk.stem.wordnet.WordNetLemmatizer"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kV6ONqbUz1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8c034269-e27c-4103-a06e-f00f467e91a3"
      },
      "source": [
        "wnl.lemmatize(\"am\", pos =\"v\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'be'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_Ab7tmyWKZm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d799b69f-15c3-4e95-f3c4-7b84f23ef774"
      },
      "source": [
        "# Now lemmatize am, is, and are. Also test what happens if you leave out the\n",
        "# pos argument? Write a comment describing what the pos argument does.\n",
        "\n",
        "# 3.5: Lemmatize am, is, and are\n",
        "print(wnl.lemmatize(\"am\", pos =\"v\"))\n",
        "print(wnl.lemmatize(\"is\", pos =\"v\"))\n",
        "print(wnl.lemmatize(\"are\", pos =\"v\"))\n",
        "\n",
        "\n",
        "# 3.6: Test the lemmatize method without the pos argument\n",
        "print(wnl.lemmatize(\"am\"))\n",
        "print(wnl.lemmatize(\"is\"))\n",
        "print(wnl.lemmatize(\"are\"))\n",
        "\n",
        "#\"pos\" defines the PART OF SPEECH of the word. for example, here were have defined \"am\" as a verb\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "be\n",
            "be\n",
            "be\n",
            "am\n",
            "is\n",
            "are\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuGJQYjMcs_a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b57c3f9-2041-42bb-b798-5c52b913567b"
      },
      "source": [
        "# Let's lemmatize Crime and Punishment to see what we get:\n",
        "crimelemma = [wnl.lemmatize(t) for t in crimetokens]\n",
        "\n",
        "len(set(crimelemma))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9793"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRndRboHc-E0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff9af8d7-a22d-45a3-a07b-7ba3f0499335"
      },
      "source": [
        "# What percentage reduction have we achieved with the lemmatizer?\n",
        "round( 100 - (len(set(crimelemma))/len(set(crimetokens)) * 100), 1)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.4"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3YJCL4tderC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c487028-2905-43e9-b09c-1f7fb1b5cf64"
      },
      "source": [
        "# Here we make a frequency distribution from crimelemma and\n",
        "# then a list of the 100 most common words in the FreqDist.\n",
        "# Then we match those words to the contents of the FreqDist\n",
        "# dictionary for the Porter stemmer. Finally, we highlight\n",
        "# tokens for which there is a mismatch in the counts.\n",
        "wdist = FreqDist(crimelemma) # Make a new FreqDist\n",
        "wlist = wdist.most_common(100) # List the 100 most common words\n",
        "\n",
        "# This creates some tuples by storing each word from wlist\n",
        "# alongside the corresponding frequency count from the Porter stemmer.\n",
        "matchlist = [(w, pdist[w[0]]) for w in wlist]\n",
        "\n",
        "# Finally, show us the list of mismatches.\n",
        "[m for m in matchlist if m[0][1] != m[1] ] # Use slicing to pull out the counts"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('a', 5865), 4651),\n",
              " (('his', 2113), 0),\n",
              " (('her', 1824), 1831),\n",
              " (('have', 1156), 1216),\n",
              " (('be', 1136), 1238),\n",
              " (('this', 724), 0),\n",
              " (('your', 675), 696),\n",
              " (('up', 644), 645),\n",
              " (('do', 591), 668),\n",
              " (('know', 574), 599),\n",
              " (('will', 552), 556),\n",
              " (('come', 495), 576),\n",
              " (('very', 466), 0),\n",
              " (('only', 458), 0),\n",
              " (('like', 457), 509),\n",
              " (('why', 445), 0),\n",
              " (('see', 393), 441),\n",
              " (('go', 369), 544),\n",
              " (('once', 346), 0),\n",
              " (('razumihin', 344), 345)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijucDSjRlIYk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aa93ba0-ab49-4c9c-a94f-2d028f95bd29"
      },
      "source": [
        "# Now compare mismatches between the top 100 lemmatized tokens with the\n",
        "# frequencies for the Lancaster stemmer and the Snowball stemmer. Based on\n",
        "# the results, add a comment saying which stemmer causes the greatest\n",
        "# number of mismatches. Can you speculate on why this might be the case?\n",
        "# Are there any additional diagnostics you could run to confirm your hypothesis?\n",
        "\n",
        "# 3.7: Compare frequencies by matching the top 100 with Lancaster\n",
        "\n",
        "wdist = FreqDist(crimelemma) # Make a new FreqDist\n",
        "wlist = wdist.most_common(100) # List the 100 most common words\n",
        "\n",
        "matchlist = [(w, ldist[w[0]]) for w in wlist]\n",
        "\n",
        "print(\"Lancaster \\n\",[m for m in matchlist if m[0][1] != m[1] ])\n",
        "\n",
        "# 3.8: Compare frequencies by matching the top 100 with Snowball\n",
        "\n",
        "matchlist = [(w, sdist[w[0]]) for w in wlist]\n",
        "\n",
        "print(\"SnowBall\\n\",[m for m in matchlist if m[0][1] != m[1] ])\n",
        "\n",
        "# 3.9: Run additional tests to understand differences between stemmers and\n",
        "# the lemmatizer."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lancaster \n",
            " [(('the', 8006), 8038), (('a', 5865), 4651), (('you', 4086), 4094), (('in', 3248), 3261), (('wa', 2826), 0), (('at', 2080), 2146), (('her', 1824), 2190), (('not', 1815), 2054), (('with', 1756), 1757), (('she', 1691), 1692), (('for', 1682), 1698), (('on', 1484), 2606), (('all', 1315), 0), (('have', 1156), 0), (('are', 868), 0), (('there', 804), 0), (('this', 724), 0), (('were', 712), 0), (('out', 679), 688), (('your', 675), 0), (('one', 663), 0), (('up', 644), 651), (('them', 584), 588), (('know', 574), 599), (('will', 552), 0), (('am', 545), 549), (('or', 506), 508), (('come', 495), 1), (('too', 491), 493), (('man', 474), 522), (('don', 464), 572), (('only', 458), 0), (('like', 457), 0), (('can', 451), 477), (('time', 441), 0), (('more', 413), 0), (('some', 405), 0), (('sonia', 399), 0), (('ha', 395), 8), (('see', 393), 444), (('go', 369), 360), (('here', 359), 0), (('once', 346), 0), (('razumihin', 344), 345)]\n",
            "SnowBall\n",
            " [(('a', 5865), 4651), (('wa', 2826), 0), (('her', 1824), 1830), (('have', 1156), 1216), (('be', 1136), 1238), (('your', 675), 696), (('up', 644), 645), (('do', 591), 668), (('know', 574), 599), (('will', 552), 556), (('come', 495), 576), (('man', 474), 475), (('very', 466), 0), (('only', 458), 0), (('like', 457), 509), (('why', 445), 0), (('ha', 395), 8), (('see', 393), 441), (('go', 369), 544), (('once', 346), 0), (('razumihin', 344), 345)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dayl1Ab3kYSv"
      },
      "source": [
        "**Part 2 - Part of Speech (POS) Tagging**\n",
        "\n",
        "At this point in the lab it should be obvious that the WordNet lemmatizer does not work very well unless we already know the part of speech of the word we are trying to lemmatize. This is a significant limitation, which is also reflected in the fact that we only achieved an 8% reduction in the number of unique tokens using this lemmatizer.\n",
        "\n",
        "Given the limitations of stemmers and simple lemmatizers, it is time to take a more serious look at part of speech tagging. For this, we are going to graduate from NLTK to our first effort with spaCy. Whereas NLTK was designed for teaching and research, spaCy was architected so that it can serve as the basis of a production-grade NLP pipeline. Unlike other NLP toolkits (e.g., Stanford core NLP) spaCy was written in Python and Cython, so it is convenient for use directly from the Jupyter notebook environment. We will do a thorough examination of many of spaCy's capabilities in a later lab session. For now, we will just try out a few techniques.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwIUu68mwY4Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0949de3-e853-46e0-fea0-e504346f0efb"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "type(nlp)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.lang.en.English"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cg5yNjGNyY9R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06e19656-db5e-47cb-8911-4e7c739c5282"
      },
      "source": [
        "# What bound methods are available?\n",
        "[m for m in dir(nlp) if m[0] != '_']"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Defaults',\n",
              " 'add_pipe',\n",
              " 'analyze_pipes',\n",
              " 'batch_size',\n",
              " 'begin_training',\n",
              " 'component',\n",
              " 'component_names',\n",
              " 'components',\n",
              " 'config',\n",
              " 'create_optimizer',\n",
              " 'create_pipe',\n",
              " 'create_pipe_from_source',\n",
              " 'default_config',\n",
              " 'default_error_handler',\n",
              " 'disable_pipe',\n",
              " 'disable_pipes',\n",
              " 'disabled',\n",
              " 'enable_pipe',\n",
              " 'evaluate',\n",
              " 'factories',\n",
              " 'factory',\n",
              " 'factory_names',\n",
              " 'from_bytes',\n",
              " 'from_config',\n",
              " 'from_disk',\n",
              " 'get_factory_meta',\n",
              " 'get_factory_name',\n",
              " 'get_pipe',\n",
              " 'get_pipe_config',\n",
              " 'get_pipe_meta',\n",
              " 'has_factory',\n",
              " 'has_pipe',\n",
              " 'initialize',\n",
              " 'lang',\n",
              " 'make_doc',\n",
              " 'max_length',\n",
              " 'meta',\n",
              " 'path',\n",
              " 'pipe',\n",
              " 'pipe_factories',\n",
              " 'pipe_labels',\n",
              " 'pipe_names',\n",
              " 'pipeline',\n",
              " 'rehearse',\n",
              " 'remove_pipe',\n",
              " 'rename_pipe',\n",
              " 'replace_listeners',\n",
              " 'replace_pipe',\n",
              " 'resume_training',\n",
              " 'select_pipes',\n",
              " 'set_error_handler',\n",
              " 'set_factory_meta',\n",
              " 'to_bytes',\n",
              " 'to_disk',\n",
              " 'tokenizer',\n",
              " 'update',\n",
              " 'use_params',\n",
              " 'vocab']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV4_95-Vy7Qg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e5532e5-bc96-4e07-81a9-0dfda56a96fe"
      },
      "source": [
        "# Let's process a small example first\n",
        "sentence = \"The faster Harry got to the store, the faster Harry would get home.\"\n",
        "spsent = nlp(sentence)\n",
        "type(spsent), len(spsent)\n",
        "print(spsent)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The faster Harry got to the store, the faster Harry would get home.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rVaDDfizZqn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b96ba95-3cbf-4228-fea5-8431b7fd0b81"
      },
      "source": [
        "# But this is no ordinary set of string tokens:\n",
        "# What bound methods and attributes are available for this object?\n",
        "[m for m in dir(spsent) if m[0] != '_']"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cats',\n",
              " 'char_span',\n",
              " 'copy',\n",
              " 'count_by',\n",
              " 'doc',\n",
              " 'ents',\n",
              " 'extend_tensor',\n",
              " 'from_array',\n",
              " 'from_bytes',\n",
              " 'from_dict',\n",
              " 'from_disk',\n",
              " 'from_docs',\n",
              " 'from_json',\n",
              " 'get_extension',\n",
              " 'get_lca_matrix',\n",
              " 'has_annotation',\n",
              " 'has_extension',\n",
              " 'has_unknown_spaces',\n",
              " 'has_vector',\n",
              " 'is_nered',\n",
              " 'is_parsed',\n",
              " 'is_sentenced',\n",
              " 'is_tagged',\n",
              " 'lang',\n",
              " 'lang_',\n",
              " 'mem',\n",
              " 'noun_chunks',\n",
              " 'noun_chunks_iterator',\n",
              " 'remove_extension',\n",
              " 'retokenize',\n",
              " 'sentiment',\n",
              " 'sents',\n",
              " 'set_ents',\n",
              " 'set_extension',\n",
              " 'similarity',\n",
              " 'spans',\n",
              " 'tensor',\n",
              " 'text',\n",
              " 'text_with_ws',\n",
              " 'to_array',\n",
              " 'to_bytes',\n",
              " 'to_dict',\n",
              " 'to_disk',\n",
              " 'to_json',\n",
              " 'to_utf8_array',\n",
              " 'user_data',\n",
              " 'user_hooks',\n",
              " 'user_span_hooks',\n",
              " 'user_token_hooks',\n",
              " 'vector',\n",
              " 'vector_norm',\n",
              " 'vocab']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wg6tH-vrzvAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dff3f177-f343-4a83-c63f-f9134a708420"
      },
      "source": [
        "# So there are quite a number of attributes and bound methods for\n",
        "# this collection of tokens. We will learn more of them eventually\n",
        "# but for now, let's just look at one attribute.\n",
        "spsent.is_tagged # What does this one tell us?"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-b51ccdc138b4>:4: DeprecationWarning: [W107] The property `Doc.is_tagged` is deprecated. Use `Doc.has_annotation(\"TAG\")` instead.\n",
            "  spsent.is_tagged # What does this one tell us?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HprBAeam5R80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08ebf2d7-a451-4827-8ac0-cdbaa3a258f1"
      },
      "source": [
        "# So spaCy has guessed the part of speech for each token. We can easily list\n",
        "# all of the tags.\n",
        "[(i, i.pos_) for i in spsent]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(The, 'PRON'),\n",
              " (faster, 'ADV'),\n",
              " (Harry, 'PROPN'),\n",
              " (got, 'VERB'),\n",
              " (to, 'ADP'),\n",
              " (the, 'DET'),\n",
              " (store, 'NOUN'),\n",
              " (,, 'PUNCT'),\n",
              " (the, 'PRON'),\n",
              " (faster, 'ADV'),\n",
              " (Harry, 'PROPN'),\n",
              " (would, 'AUX'),\n",
              " (get, 'VERB'),\n",
              " (home, 'ADV'),\n",
              " (., 'PUNCT')]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zLxoqQH6HJf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ac91e5a-6875-4c7e-a2fb-6d40a9b5e1a0"
      },
      "source": [
        "# SpaCy has also stored the lemmas for each token\n",
        "# Let's show the lemmas and clean up our output. We can use the\n",
        "# tabulate package to make simple display tables.\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Make a little dataset for tabulate() to work on.\n",
        "poslist = [ (i.text, i.lemma_, i.pos_) for i in spsent]\n",
        "\n",
        "print(tabulate(poslist,  headers=[\"Token\", \"Lemma\", \"Tag\"]))\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token    Lemma    Tag\n",
            "-------  -------  -----\n",
            "The      the      PRON\n",
            "faster   fast     ADV\n",
            "Harry    Harry    PROPN\n",
            "got      get      VERB\n",
            "to       to       ADP\n",
            "the      the      DET\n",
            "store    store    NOUN\n",
            ",        ,        PUNCT\n",
            "the      the      PRON\n",
            "faster   fast     ADV\n",
            "Harry    Harry    PROPN\n",
            "would    would    AUX\n",
            "get      get      VERB\n",
            "home     home     ADV\n",
            ".        .        PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZo9P_EJoUsr"
      },
      "source": [
        "The table above just scratches the surface, but there's still a lot of interesting stuff happening there. In the first column we have the token itself, which can be a word, a number, or punctuation. The second column has the lemma and the last is the simple part of speech tag. By the way, you can find an explanation of these tages here:\n",
        "\n",
        "https://universaldependencies.org/docs/u/pos/\n",
        "\n",
        "There is a function call that will provide information about any of the tags:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJA12TMdoI1C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "047b693a-d12d-4e94-ab3a-4fa4d2de5a8c"
      },
      "source": [
        "spacy.explain(\"DET\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'determiner'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjmOSyCrr5WE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b046a41d-6fcb-4051-bf2f-1c453dc7377d"
      },
      "source": [
        "# Let's practice by tagging another sentence. Here's some text extracted from\n",
        "# Wikipedia's article on kites.\n",
        "kites = \"\"\"\n",
        "A kite is a tethered heavier-than-air or lighter-than-air craft with wing surfaces that react against the air to create lift and drag forces.\n",
        "A kite consists of wings, tethers and anchors. Kites often have a bridle and tail to guide the face of the kite so the wind can lift it.\n",
        "Some kite designs don’t need a bridle; box kites can have a single attachment point.\n",
        "A kite may have fixed or moving anchors that can balance the kite.\n",
        "One technical definition is that a kite is “a collection of tether-coupled wing sets“.\n",
        "The name derives from its resemblance to a hovering bird.\n",
        "\"\"\"\n",
        "\n",
        "spkites = nlp(kites)\n",
        "type(spkites), len(spkites)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(spacy.tokens.doc.Doc, 132)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTE4tWNIsoFd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f810d22-c7f0-4b50-b855-3d7e03c98928"
      },
      "source": [
        "# Add code to conduct the following analyses:\n",
        "\n",
        "# 3.10: Display tokens, lemmas, and parts of speech for spkites. Try using a\n",
        "# nice, neat tabular format for the output.\n",
        "\n",
        "poslist = [ (i.text, i.lemma_, i.pos_) for i in spkites]\n",
        "\n",
        "print(tabulate(poslist,  headers=[\"Token\", \"Lemma\", \"Tag\"]))\n",
        "\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token        Lemma        Tag\n",
            "-----------  -----------  -----\n",
            "                          SPACE\n",
            "A            a            DET\n",
            "kite         kite         NOUN\n",
            "is           be           AUX\n",
            "a            a            DET\n",
            "tethered     tethered     ADJ\n",
            "heavier      heavy        ADJ\n",
            "-            -            PUNCT\n",
            "than         than         ADP\n",
            "-            -            PUNCT\n",
            "air          air          NOUN\n",
            "or           or           CCONJ\n",
            "lighter      light        ADJ\n",
            "-            -            PUNCT\n",
            "than         than         ADP\n",
            "-            -            PUNCT\n",
            "air          air          NOUN\n",
            "craft        craft        NOUN\n",
            "with         with         ADP\n",
            "wing         wing         NOUN\n",
            "surfaces     surface      NOUN\n",
            "that         that         PRON\n",
            "react        react        VERB\n",
            "against      against      ADP\n",
            "the          the          DET\n",
            "air          air          NOUN\n",
            "to           to           PART\n",
            "create       create       VERB\n",
            "lift         lift         NOUN\n",
            "and          and          CCONJ\n",
            "drag         drag         NOUN\n",
            "forces       force        NOUN\n",
            ".            .            PUNCT\n",
            "                          SPACE\n",
            "A            a            DET\n",
            "kite         kite         NOUN\n",
            "consists     consist      VERB\n",
            "of           of           ADP\n",
            "wings        wing         NOUN\n",
            ",            ,            PUNCT\n",
            "tethers      tether       NOUN\n",
            "and          and          CCONJ\n",
            "anchors      anchor       NOUN\n",
            ".            .            PUNCT\n",
            "Kites        kite         NOUN\n",
            "often        often        ADV\n",
            "have         have         VERB\n",
            "a            a            DET\n",
            "bridle       bridle       NOUN\n",
            "and          and          CCONJ\n",
            "tail         tail         VERB\n",
            "to           to           PART\n",
            "guide        guide        VERB\n",
            "the          the          DET\n",
            "face         face         NOUN\n",
            "of           of           ADP\n",
            "the          the          DET\n",
            "kite         kite         NOUN\n",
            "so           so           SCONJ\n",
            "the          the          DET\n",
            "wind         wind         NOUN\n",
            "can          can          AUX\n",
            "lift         lift         VERB\n",
            "it           it           PRON\n",
            ".            .            PUNCT\n",
            "                          SPACE\n",
            "Some         some         DET\n",
            "kite         kite         NOUN\n",
            "designs      design       NOUN\n",
            "do           do           AUX\n",
            "n’t          not          PART\n",
            "need         need         VERB\n",
            "a            a            DET\n",
            "bridle       bridle       NOUN\n",
            ";            ;            PUNCT\n",
            "box          box          NOUN\n",
            "kites        kite         NOUN\n",
            "can          can          AUX\n",
            "have         have         VERB\n",
            "a            a            DET\n",
            "single       single       ADJ\n",
            "attachment   attachment   NOUN\n",
            "point        point        NOUN\n",
            ".            .            PUNCT\n",
            "                          SPACE\n",
            "A            a            DET\n",
            "kite         kite         NOUN\n",
            "may          may          AUX\n",
            "have         have         AUX\n",
            "fixed        fix          VERB\n",
            "or           or           CCONJ\n",
            "moving       move         VERB\n",
            "anchors      anchor       NOUN\n",
            "that         that         PRON\n",
            "can          can          AUX\n",
            "balance      balance      VERB\n",
            "the          the          DET\n",
            "kite         kite         NOUN\n",
            ".            .            PUNCT\n",
            "                          SPACE\n",
            "One          one          NUM\n",
            "technical    technical    ADJ\n",
            "definition   definition   NOUN\n",
            "is           be           AUX\n",
            "that         that         SCONJ\n",
            "a            a            DET\n",
            "kite         kite         NOUN\n",
            "is           be           AUX\n",
            "“            \"            PUNCT\n",
            "a            a            DET\n",
            "collection   collection   NOUN\n",
            "of           of           ADP\n",
            "tether       tether       NOUN\n",
            "-            -            PUNCT\n",
            "coupled      couple       VERB\n",
            "wing         wing         NOUN\n",
            "sets         set          NOUN\n",
            "“            \"            PUNCT\n",
            ".            .            PUNCT\n",
            "                          SPACE\n",
            "The          the          DET\n",
            "name         name         NOUN\n",
            "derives      derive       VERB\n",
            "from         from         ADP\n",
            "its          its          PRON\n",
            "resemblance  resemblance  NOUN\n",
            "to           to           ADP\n",
            "a            a            DET\n",
            "hovering     hover        VERB\n",
            "bird         bird         NOUN\n",
            ".            .            PUNCT\n",
            "                          SPACE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSdCtkO-uvX7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aafbd4a0-094d-4f36-eaf5-403736a57537"
      },
      "source": [
        "# It might be more convenient to work with individual sentences:\n",
        "kitespans = list(spkites.sents)\n",
        "\n",
        "kitespans[0] # Let's view just the first sentence"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "A kite is a tethered heavier-than-air or lighter-than-air craft with wing surfaces that react against the air to create lift and drag forces."
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbgHBwm66EM_"
      },
      "source": [
        "Let's close the loop on the idea of data reduction by seeing how many unique lemmas spaCy creates for Crime and Punishment. Recall that we were unsatisfied with the lemmatizer from NLTK because - in order for it to work efficiently - we needed to know the POS for each token before calling the lemmatizer. The spaCy nlp() call east up our whole text, applies tags, and determines lemmas, all based on a swappable language model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t6ne0lS6A1N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "124f31ad-44c8-4e48-a4b7-d585563ccf4d"
      },
      "source": [
        "# Process Crime and Punishment with spaCy\n",
        "nlp.max_length = 1200000 # Increase from the default of 1 million characters\n",
        "\n",
        "# Note that this call takes a little less than a minute to complete.\n",
        "crimespacy = nlp(raw) # We're going back to the original raw text data!\n",
        "\n",
        "type(crimespacy), len(crimespacy)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(spacy.tokens.doc.Doc, 274697)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-BTBbW-7_nZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39c5a361-5a1e-4516-d837-294d9d81875a"
      },
      "source": [
        "# Let's count unique lemmas\n",
        "newcrimelemma = [l.lemma_ for l in crimespacy]\n",
        "len(set(newcrimelemma))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7858"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRC0_cIT8mnW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ee65f9f-64e7-4ee8-e921-bb42ebae6754"
      },
      "source": [
        "# What percentage reduction have we achieved with the lemmatizer?\n",
        "round( 100 - (len(set(newcrimelemma))/len(set(crimetokens)) * 100), 1)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26.5"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-5oEhkb8141"
      },
      "source": [
        "**Part 3 - WordNet**\n",
        "\n",
        "The leading definition of the word semantics is, \"the study of the meaning of words.\"\n",
        "\n",
        "One of the earliest and most comprehensive efforts to explore semantics on a large scale arose from the work of George Miller at Princeton in the mid-1980s. The database arising from Miller's work, known as WordNet, was an award-winning effort to create a network of interconnected meanings of words. The WordNet project is alive and well in the present day, in fact there is an international organization  known as the Global WordNet Association that continues research and development of WordNet. Check it out here:\n",
        "\n",
        "http://globalwordnet.org\n",
        "\n",
        "GWA has an annual conference and offers some databases and documentation to the world community for free. These databases, now covering more than 200 languages, represent a massive amount of collective human effort, which is both amazing and illustrative of the core challenge with such resources: The maintenance of manually developed language resources requires lots of manual labor.\n",
        "\n",
        "Possibly, some of the value of what WordNet provides has been or will eventually be superceded by approaches based on deep learning. We see inklings of this with GloVe word embedding and more sophisticated embedding approaches such as BERT that are initially trained (in an unsupervised mode) on masses of unlabeled natural language text. Even so, having some understanding of how WordNet works and what it can do will set the stage for understanding newer approachs. So in this part of the lab, we explore some of the WordNet capabilities afforded by NLTK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUKzxUVv31MY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d7a7d2b-7b81-423a-93b3-c899ce1efb14"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet') # Colab does not have it installed by default\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "type(wn.synsets) # A key function call (method) that we will use"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "method"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQXUKYTC4G1R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37e83f11-3fa5-49ea-9183-096df8a7e787"
      },
      "source": [
        "# Let's start by getting data on the word cat. A \"synset\" is a very basic\n",
        "# data structure supported by NLTK that can be used to look up synonyms\n",
        "# and related information for any word that the WordNet folks have included\n",
        "# in the giant database.\n",
        "syn = wn.synsets('cat')\n",
        "type(syn), len(syn)\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(list, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeJqwuhC3xGB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e5fa97b-2eda-4482-f67a-27c704fb5b0b"
      },
      "source": [
        "# Each element in the list is a synset object. We have more than one whenever\n",
        "# there is more than one sense of the word.\n",
        "\n",
        "cat0 = syn[0] # Let's look at some of the details for the first synset\n",
        "\n",
        "print (\"Synset name :  \", cat0.name())\n",
        "\n",
        "# Defining the word\n",
        "print (\"\\nSynset meaning : \", cat0.definition())\n",
        "\n",
        "# list of phrases that use the word in context; not all words have these\n",
        "print (\"\\nSynset example : \", cat0.examples())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset name :   cat.n.01\n",
            "\n",
            "Synset meaning :  feline mammal usually having thick soft fur and no ability to roar: domestic cats; wildcats\n",
            "\n",
            "Synset example :  []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the synset name has interesting information in it: Of course the word itself comes first, but then the letter after the dot indicates the part of speech. The number after the second dot reveals the variant. So cat.n.01 would be read as \"the first noun sense of cat.\" The fact that cat.n.01 appears as the first synset in the list indicates that linguists believed it to be the most common sense of the word.\n",
        "\n",
        "WordNet is organized as a kind of tree structure, where we can find more specific and more general terms related to a particular word by tracing up or down the branches and twigs of the tree. A \"hypernym\" - which you can think of as \"higher level name\" - is a more general term that encompasses the word we are focusing on. In the other direction, a \"hyponym\" is an example of a word that is more specific than the word we are focusing on. As a mnemonic, remember that \"hyper\" means \"excess\" or \"above\" as in \"hyperactive.\" On the other hand, \"hypo\" means below, as in \"hypothermia.\""
      ],
      "metadata": {
        "id": "D68-dHKGVuEr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7s6PD_K86Q9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95d4b4a2-c32b-4971-a9c2-717e797ca7c2"
      },
      "source": [
        "print (\"Synset name :  \", cat0.name()) # Let's show the name again\n",
        "\n",
        "# Here is the \"root\" word - the highest level hypernym\n",
        "print (\"\\nSynset root hypernym:  \", cat0.root_hypernyms())\n",
        "\n",
        "# These are the more general terms\n",
        "print (\"\\nSynset hypernyms:  \", cat0.hypernyms())\n",
        "\n",
        "# These are the more specific terms\n",
        "print (\"\\nSynset hyponyms:  \", cat0.hyponyms())\n",
        "\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset name :   cat.n.01\n",
            "\n",
            "Synset root hypernym:   [Synset('entity.n.01')]\n",
            "\n",
            "Synset hypernyms:   [Synset('feline.n.01')]\n",
            "\n",
            "Synset hyponyms:   [Synset('domestic_cat.n.01'), Synset('wildcat.n.03')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyWNgAoDmgDw"
      },
      "source": [
        "The second and subsequent elements in the synset list (if any) are alternative word senses. If you're a music fan, you might be able to think of another use of the word \"cat.\" In the first line of code below, we extract the second element of the synset list. Use it to show the name, definition, example, root hypernym, hypernyms, and hyponyms for this first synonym of cat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48e3WIGn97lq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd83a537-73a9-4531-82bd-eae679190311"
      },
      "source": [
        "# Exercises: Explore the second synset for \"cat.\"\n",
        "\n",
        "cat1 = syn[1] # Let's look at some of the details for the second element\n",
        "\n",
        "# 3.11: Print the name of cat1: What part of speech is it?\n",
        "print (\"Synset name :  \", cat1.name())\n",
        "#NOUN\n",
        "\n",
        "# 3.12: Print the definition of cat1, the examples of use of cat1 in context, the root hypernym of cat1, a list of hypernyms of cat1, and a list of hyponyms of cat1\n",
        "\n",
        "print (\"\\nSynset meaning : \", cat1.definition())\n",
        "print (\"\\nSynset root hypernym:  \", cat1.root_hypernyms())\n",
        "print (\"\\nSynset hypernyms:  \", cat1.hypernyms())\n",
        "print (\"\\nSynset hyponyms:  \", cat1.hyponyms())\n",
        "\n",
        "\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset name :   guy.n.01\n",
            "\n",
            "Synset meaning :  an informal term for a youth or man\n",
            "\n",
            "Synset root hypernym:   [Synset('entity.n.01')]\n",
            "\n",
            "Synset hypernyms:   [Synset('man.n.01')]\n",
            "\n",
            "Synset hyponyms:   [Synset('sod.n.04')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwNfii4hAD-Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3001c77e-6cf3-4af3-e3f1-555b3e4b3394"
      },
      "source": [
        "# Given what you saw above, does it make sense now why the root hypernym\n",
        "# of cat is \"entity\" rather than something more specific like \"animal?\"\n",
        "\n",
        "# Cat is such a common word in English that it has been reused to refer\n",
        "# to many different kinds of things. Let's go back to the complete list\n",
        "# to show all of the definitions:\n",
        "\n",
        "[s.definition() for s in syn]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['feline mammal usually having thick soft fur and no ability to roar: domestic cats; wildcats',\n",
              " 'an informal term for a youth or man',\n",
              " 'a spiteful woman gossip',\n",
              " 'the leaves of the shrub Catha edulis which are chewed like tobacco or used to make tea; has the effect of a euphoric stimulant',\n",
              " 'a whip with nine knotted cords',\n",
              " 'a large tracked vehicle that is propelled by two endless metal belts; frequently used for moving earth in construction and farm work',\n",
              " 'any of several large cats typically able to roar and living in the wild',\n",
              " 'a method of examining body organs by scanning them with X rays and using a computer to construct a series of cross-sectional scans along a single axis',\n",
              " \"beat with a cat-o'-nine-tails\",\n",
              " 'eject the contents of the stomach through the mouth']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jlp3992_BA4g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62c68bd8-ffc5-4c3e-897b-ffe6998416f1"
      },
      "source": [
        "# That's an amazing variety. Let's also glue the corresponding synset name\n",
        "# to the definition so that we can see the parts of speech and numbering.\n",
        "[ (s.name(), s.definition())  for s in syn]"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cat.n.01',\n",
              "  'feline mammal usually having thick soft fur and no ability to roar: domestic cats; wildcats'),\n",
              " ('guy.n.01', 'an informal term for a youth or man'),\n",
              " ('cat.n.03', 'a spiteful woman gossip'),\n",
              " ('kat.n.01',\n",
              "  'the leaves of the shrub Catha edulis which are chewed like tobacco or used to make tea; has the effect of a euphoric stimulant'),\n",
              " (\"cat-o'-nine-tails.n.01\", 'a whip with nine knotted cords'),\n",
              " ('caterpillar.n.02',\n",
              "  'a large tracked vehicle that is propelled by two endless metal belts; frequently used for moving earth in construction and farm work'),\n",
              " ('big_cat.n.01',\n",
              "  'any of several large cats typically able to roar and living in the wild'),\n",
              " ('computerized_tomography.n.01',\n",
              "  'a method of examining body organs by scanning them with X rays and using a computer to construct a series of cross-sectional scans along a single axis'),\n",
              " ('cat.v.01', \"beat with a cat-o'-nine-tails\"),\n",
              " ('vomit.v.01', 'eject the contents of the stomach through the mouth')]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNFyf-zrC3WN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a327635c-9062-417e-9cb3-3d08910c4d57"
      },
      "source": [
        "# That last one is British slang, probably arising from the propensity of\n",
        "# domestic cats to retch hairballs. Anyway. . . We can also get lemmas for\n",
        "# each synonym entry in our list of 10:\n",
        "[ (s.name(), s.lemma_names())  for s in syn]"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cat.n.01', ['cat', 'true_cat']),\n",
              " ('guy.n.01', ['guy', 'cat', 'hombre', 'bozo']),\n",
              " ('cat.n.03', ['cat']),\n",
              " ('kat.n.01',\n",
              "  ['kat', 'khat', 'qat', 'quat', 'cat', 'Arabian_tea', 'African_tea']),\n",
              " (\"cat-o'-nine-tails.n.01\", [\"cat-o'-nine-tails\", 'cat']),\n",
              " ('caterpillar.n.02', ['Caterpillar', 'cat']),\n",
              " ('big_cat.n.01', ['big_cat', 'cat']),\n",
              " ('computerized_tomography.n.01',\n",
              "  ['computerized_tomography',\n",
              "   'computed_tomography',\n",
              "   'CT',\n",
              "   'computerized_axial_tomography',\n",
              "   'computed_axial_tomography',\n",
              "   'CAT']),\n",
              " ('cat.v.01', ['cat']),\n",
              " ('vomit.v.01',\n",
              "  ['vomit',\n",
              "   'vomit_up',\n",
              "   'purge',\n",
              "   'cast',\n",
              "   'sick',\n",
              "   'cat',\n",
              "   'be_sick',\n",
              "   'disgorge',\n",
              "   'regorge',\n",
              "   'retch',\n",
              "   'puke',\n",
              "   'barf',\n",
              "   'spew',\n",
              "   'spue',\n",
              "   'chuck',\n",
              "   'upchuck',\n",
              "   'honk',\n",
              "   'regurgitate',\n",
              "   'throw_up'])]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWkVa6JbDWgh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce16315f-b8b7-4b30-8cda-d8e7052fdad4"
      },
      "source": [
        "# The elements of each of list shown above (as the second part of the tuple)\n",
        "# are plain words representing the synonym set. This could come in handy\n",
        "# later, so let's make sure we know how to extract each synonym\n",
        "\n",
        "[s.lemma_names()[0] for s in syn]"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cat',\n",
              " 'guy',\n",
              " 'cat',\n",
              " 'kat',\n",
              " \"cat-o'-nine-tails\",\n",
              " 'Caterpillar',\n",
              " 'big_cat',\n",
              " 'computerized_tomography',\n",
              " 'cat',\n",
              " 'vomit']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4K8rcg7FMFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3e1ec06-4bcc-4a10-fdef-79dc7376521b"
      },
      "source": [
        "# Now repeat the process by finding the synset for an adjectival word, like\n",
        "# good, bad, great, horrid. etc. Show the list of lemma names for that word.\n",
        "# As a related task, reduce that list of lemma names to its unique set\n",
        "# in order to eliminate duplicates. As a bonus challenge, can you figure out\n",
        "# how to do all that with just one line of code?\n",
        "\n",
        "# 3.13: Generate a unique set of lemmas for an adjective of your choice.\n",
        "\n",
        "deliciousSyn = wn.synsets('Delicious')\n",
        "[list(s.lemma_names()) for s in deliciousSyn]\n",
        "\n",
        "\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Delicious'],\n",
              " ['delightful', 'delicious'],\n",
              " ['delectable',\n",
              "  'delicious',\n",
              "  'luscious',\n",
              "  'pleasant-tasting',\n",
              "  'scrumptious',\n",
              "  'toothsome',\n",
              "  'yummy']]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2GB7qH1KN9D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08ddfb3c-9e10-4ae2-fd9e-ec0f65dccc15"
      },
      "source": [
        "# There are a couple more useful things we can do with a synset. First, we can\n",
        "# ask WordNet for the part of speech for each entry:\n",
        "from tabulate import tabulate # To make a neat table\n",
        "\n",
        "takesyn = wn.synsets('take') # The word \"take\" has many senses - noun and verb\n",
        "\n",
        "poslist = [(s.lemma_names()[0], s.pos(), s.definition()) for s in takesyn]\n",
        "\n",
        "print(tabulate(poslist,  headers=[\"Word\", \"POS\", \"Definition\"]))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word         POS    Definition\n",
            "-----------  -----  ----------------------------------------------------------------------------------------------\n",
            "return       n      the income or profit arising from such transactions as the sale of land or other property\n",
            "take         n      the act of photographing a scene or part of a scene without interruption\n",
            "take         v      carry out\n",
            "take         v      require (time or space)\n",
            "lead         v      take somebody somewhere\n",
            "take         v      get into one's hands, take physically\n",
            "assume       v      take on a certain form, attribute, or aspect\n",
            "take         v      interpret something in a certain way; convey a particular meaning or impression\n",
            "bring        v      take something or somebody with oneself somewhere\n",
            "take         v      take into one's possession\n",
            "take         v      travel or go by means of a certain kind of transportation, or a certain route\n",
            "choose       v      pick out, select, or choose from a number of alternatives\n",
            "accept       v      receive willingly something given or offered\n",
            "fill         v      assume, as of positions or roles\n",
            "consider     v      take into consideration for exemplifying purposes\n",
            "necessitate  v      require as useful, just, or proper\n",
            "take         v      experience or feel or submit to\n",
            "film         v      make a film or photograph of something\n",
            "remove       v      remove something concrete, as by lifting, pushing, or taking off, or remove something abstract\n",
            "consume      v      serve oneself to, or consume regularly\n",
            "take         v      accept or undergo, often unwillingly\n",
            "take         v      make use of or accept for some purpose\n",
            "take         v      take by force\n",
            "assume       v      occupy or take on\n",
            "accept       v      admit into a group or community\n",
            "take         v      ascertain or determine by measuring, computing or take a reading from a dial\n",
            "learn        v      be a student of a certain subject\n",
            "claim        v      take as an undesirable consequence of some event or state of affairs\n",
            "take         v      head into a specified direction\n",
            "aim          v      point or cause to go (blows, weapons, or objects such as photographic equipment) towards\n",
            "take         v      be seized or affected in a specified way\n",
            "carry        v      have with oneself; have on one's person\n",
            "lease        v      engage for service under a term of contract\n",
            "subscribe    v      receive or obtain regularly\n",
            "take         v      buy, select\n",
            "take         v      to get into a position of having, e.g., safety, comfort\n",
            "take         v      have sex with; archaic use\n",
            "claim        v      lay claim to; as of an idea\n",
            "accept       v      be designed to hold or take\n",
            "contain      v      be capable of holding or containing\n",
            "take         v      develop a habit\n",
            "drive        v      proceed along in a vehicle\n",
            "take         v      obtain by winning\n",
            "contract     v      be stricken by an illness, fall victim to an illness\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c4Xnf27-4O0"
      },
      "source": [
        "Having all of the most common words in a language organized based on their hypernyms and hyponyms leads to some interesting results. For example, the noun senses of \"dog\" and \"cat\" that refer to pets both have mammal as a \"container\" word. So we can traverse our way upward from \"cat\" to find the common ancestor word and then traverse back down to \"dog.\" If we started with \"cat\" and we wanted to get to \"doctor\" it would probably take a lot more steps, because the common ancestor word would be much more general.    \n",
        "\n",
        "This leads to an interesting possibility: We can calculate the similarity between any pair of words by measuring the length of the \"path\" along the twigs and branches that connects two words. Here's an example to illustrate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2M0wbIHrUjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e16e8889-b1b4-4bc7-9970-216f3d65c7c5"
      },
      "source": [
        "# Pay close attention: the \"synset\" method looks up ONE synset if it\n",
        "# exists. We have to specify exactly which synset we are talking about,\n",
        "# so that's why we use something like bird.n.01 to refer to the first\n",
        "# noun sense of bird. Earlier in this lab we used the \"synsets\" method\n",
        "# which will look up all of the available synsets for a word. So \"synset\"\n",
        "# and \"synsets\" do slightly different jobs.\n",
        "birdsyn = wn.synset('bird.n.01')\n",
        "goatsyn = wn.synset('goat.n.01')\n",
        "sheepsyn = wn.synset('sheep.n.01')\n",
        "\n",
        "birdsyn.path_similarity(goatsyn) # Bird to goat\n",
        "# These distances are normalized to be on a scale of 0 to 1 where 0\n",
        "# is least similar and 1 is most similar.\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1111111111111111"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdlWtyc9tFAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aead57a-f6fe-42a5-cce4-309f035a7f83"
      },
      "source": [
        "# Does this value make sense?\n",
        "birdsyn.path_similarity(sheepsyn) # Bird to sheep"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1111111111111111"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpSb4pNOtoYB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d0bd278-ca8b-4b60-b817-fb2269ff3af2"
      },
      "source": [
        "# How about goat to sheep?\n",
        "goatsyn.path_similarity(sheepsyn)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3333333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTddgazJwb-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cab2e891-67f8-4fe2-864d-a7191d093eba"
      },
      "source": [
        "# As with many things related to language, there is often an alternative way\n",
        "# to do something. Leacock-Chodorow similarity also uses the path lengths,\n",
        "# but also uses how deep the least common ancestor is in the hierarchy.\n",
        "# Resnik similarity also considers the relative frequency of a word in a\n",
        "# corpus you provide. We repeat the display of path similarity here just\n",
        "# for the sake of comparison.\n",
        "nltk.download('wordnet_ic')\n",
        "from nltk.corpus import wordnet_ic\n",
        "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
        "\n",
        "birdsyn.path_similarity(goatsyn), birdsyn.lch_similarity(goatsyn), birdsyn.res_similarity(goatsyn, brown_ic)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet_ic.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.1111111111111111, 1.4403615823901665, 5.2175784741185165)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1VsAeR6xmTN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f346c31-ad85-4006-d95f-d43d417f75e8"
      },
      "source": [
        "# Obviously, these other similarity measures are calibrated on different\n",
        "# scales from path similarity. Add code to produce the Leacock-Chodorow\n",
        "# and the Resnick similarity for the other two pairs, specifically:\n",
        "# birdsyn to sheepsyn, and\n",
        "# sheepsyn to goatsyn\n",
        "\n",
        "# 3.14: Compute L-C and Res similarity for birdsyn to sheepsyn\n",
        "\n",
        "print(\"Bird - Sheep (lch_similarity): \",birdsyn.lch_similarity(sheepsyn)), print(\"Bird - Sheep (res_similarity): \",birdsyn.res_similarity(sheepsyn, brown_ic))\n",
        "print(\"sheep - goat (lch_similarity): \",sheepsyn.lch_similarity(goatsyn)), print(\"sheep - goat (res_similarity): \",sheepsyn.res_similarity(goatsyn, brown_ic))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bird - Sheep (lch_similarity):  1.4403615823901665\n",
            "Bird - Sheep (res_similarity):  5.2175784741185165\n",
            "sheep - goat (lch_similarity):  2.538973871058276\n",
            "sheep - goat (res_similarity):  8.005695458684853\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIsWfofPyiui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73921adf-19c5-4164-c3a4-6c62c6f89cb2"
      },
      "source": [
        "# OK, one final WordNet trick: Antonyms. If we want to find a word with\n",
        "# the opposite meaning, WordNet can provide us with choices:\n",
        "syn = wn.synsets('good') # Grab all of the synonyms for good\n",
        "[(s.name(), s.definition()) for s in syn] # Display them"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('good.n.01', 'benefit'),\n",
              " ('good.n.02', 'moral excellence or admirableness'),\n",
              " ('good.n.03', 'that which is pleasing or valuable or useful'),\n",
              " ('commodity.n.01', 'articles of commerce'),\n",
              " ('good.a.01',\n",
              "  'having desirable or positive qualities especially those suitable for a thing specified'),\n",
              " ('full.s.06', 'having the normally expected amount'),\n",
              " ('good.a.03', 'morally admirable'),\n",
              " ('estimable.s.02', 'deserving of esteem and respect'),\n",
              " ('beneficial.s.01', 'promoting or enhancing well-being'),\n",
              " ('good.s.06', 'agreeable or pleasing'),\n",
              " ('good.s.07', 'of moral excellence'),\n",
              " ('adept.s.01', 'having or showing knowledge and skill and aptitude'),\n",
              " ('good.s.09', 'thorough'),\n",
              " ('dear.s.02', 'with or in a close or intimate relationship'),\n",
              " ('dependable.s.04', 'financially sound'),\n",
              " ('good.s.12', 'most suitable or right for a particular purpose'),\n",
              " ('good.s.13', 'resulting favorably'),\n",
              " ('effective.s.04', 'exerting force or influence'),\n",
              " ('good.s.15', 'capable of pleasing'),\n",
              " ('good.s.16', 'appealing to the mind'),\n",
              " ('good.s.17', 'in excellent physical condition'),\n",
              " ('good.s.18', 'tending to promote physical well-being; beneficial to health'),\n",
              " ('good.s.19', 'not forged'),\n",
              " ('good.s.20', 'not left to spoil'),\n",
              " ('good.s.21', 'generally admired'),\n",
              " ('well.r.01',\n",
              "  \"(often used as a combining form) in a good or proper or satisfactory manner or to a high standard (`good' is a nonstandard dialectal variant for `well')\"),\n",
              " ('thoroughly.r.02',\n",
              "  \"completely and absolutely (`good' is sometimes used informally for `thoroughly')\")]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24ghhlidz7cH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06cc9de6-0e5b-43b1-974e-58eacf73ed20"
      },
      "source": [
        "# Let's use the first adjectival form:\n",
        "goodsyn = wn.synset('good.a.01')\n",
        "\n",
        "# Now get the antonym from the lemma\n",
        "[l.antonyms() for l in goodsyn.lemmas()]"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[Lemma('bad.a.01.bad')]]"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U88lc1L024RT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe8d8e2b-fc1b-4589-be5a-a7046ad082c3"
      },
      "source": [
        "# Now you look up the antonym(s) for the adjectival sense of bad.\n",
        "\n",
        "# 3.15: Look up the antonym for bad\n",
        "# Let's use the first adjectival form:\n",
        "badsyn = wn.synset('bad.a.01')\n",
        "\n",
        "# Now get the antonym from the lemma\n",
        "[l.antonyms() for l in badsyn.lemmas()]\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[Lemma('good.a.01.good')]]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    }
  ]
}