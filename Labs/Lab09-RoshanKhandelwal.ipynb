{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22dkFCBRK3qX"
      },
      "source": [
        "**IST664 - NLP Lab Week 9**\n",
        "\n",
        "In previous lab, we have practiced different word embedding techniques to convert text data into vectors of numeric data. In this lab, we will learn to use convolutional neural network (CNN) for recognizing emotions in a tweet. The algorithm we learn uses multiple channels - the raw tweet text, the hash tags, emojis, and emoticons, and the features based on the emotion lexicons. This is an example that shows the potential to combine deep learning techniques and the language processing knowledge we gain from the foundations. For this lab, you will need the lexicon resources which are available in this week's folder on the course site.\n",
        "\n",
        "Please note that the multi-channel CNN algorithm below is developed by Islam, Mercer, & Xiao (2019). If you plan to use it for your data analysis work, please cite the references to give the researchers credit. Thank you.\n",
        "\n",
        "Islam, J., Mercer, R. E., & Xiao, L. (2019, June). Multi-channel convolutional neural network for twitter emotion and sentiment recognition. In Proceedings of the 2019 Conference of the North American Chapter of the Association for [link text](https://)Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) (pp. 1355-1365)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9_wpiL0K5px",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00ab0344-dc8d-402f-ce5c-3b07debe3b2e"
      },
      "source": [
        "!pip install emoji\n",
        "!pip install vaderSentiment"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.9.0-py2.py3-none-any.whl (397 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.5/397.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.9.0\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2023.11.17)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ayg_drolLEk3"
      },
      "source": [
        "# ********* Import Packages ********* #\n",
        "import re, csv, emoji, operator\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import numpy as np\n",
        "from numpy import asarray, zeros, array\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import KFold\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "#from emoji.unicode_codes import UNICODE_EMOJI\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "#from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Dropout, Embedding, GlobalMaxPooling1D\n",
        "from keras.layers import Conv1D\n",
        "#from keras.layers.merge import concatenate\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from keras.callbacks import Callback"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQc3UXOaLJMm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3f61ce0-ab00-4a3d-f3e5-f1d46b4b34fb"
      },
      "source": [
        "# ********* Load Data ********* #\n",
        "# Important: Change load_data() function according to the dataset that you have. The following\n",
        "# function processes the Twitter Emotion Corpus (TEC)\n",
        "# Link of paper: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.383.3384&rep=rep1&type=pdf\n",
        "# Link of dataset: http://saifmohammad.com/WebPages/SentimentEmotionLabeledData.html\n",
        "\n",
        "basefile = \"Jan9-2012-tweets-clean.txt\"\n",
        "\n",
        "# This downloads the zip file\n",
        "!wget http://saifmohammad.com/WebDocs/Jan9-2012-tweets-clean.txt.zip\n",
        "!unzip Jan9-2012-tweets-clean.txt.zip\n",
        "\n",
        "# List of tweets\n",
        "texts = []\n",
        "\n",
        "# List of labels\n",
        "labels = []\n",
        "\n",
        "def load_data():\n",
        "    with open(basefile, 'r', encoding=\"utf8\") as f:\n",
        "        for line in f:\n",
        "            splitted = line.strip().split()\n",
        "            labels.append(splitted[len(splitted)-1])\n",
        "            texts.append(' '.join(splitted[1:len(splitted)-2]))\n",
        "    print('Loaded %s  data' % len(labels))\n",
        "\n",
        "print(\"Loading data...\")\n",
        "load_data()\n",
        "print(Counter(labels))\n",
        "\n",
        "# Example\n",
        "print(texts[55])\n",
        "print(labels[55])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-11 16:50:54--  http://saifmohammad.com/WebDocs/Jan9-2012-tweets-clean.txt.zip\n",
            "Resolving saifmohammad.com (saifmohammad.com)... 192.185.17.122\n",
            "Connecting to saifmohammad.com (saifmohammad.com)|192.185.17.122|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1128895 (1.1M) [application/zip]\n",
            "Saving to: ‘Jan9-2012-tweets-clean.txt.zip’\n",
            "\n",
            "\r          Jan9-2012   0%[                    ]       0  --.-KB/s               \rJan9-2012-tweets-cl 100%[===================>]   1.08M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2024-01-11 16:50:54 (12.3 MB/s) - ‘Jan9-2012-tweets-clean.txt.zip’ saved [1128895/1128895]\n",
            "\n",
            "Archive:  Jan9-2012-tweets-clean.txt.zip\n",
            "  inflating: Jan9-2012-tweets-clean.txt  \n",
            "   creating: __MACOSX/\n",
            "  inflating: __MACOSX/._Jan9-2012-tweets-clean.txt  \n",
            "Loading data...\n",
            "Loaded 21051  data\n",
            "Counter({'joy': 8240, 'surprise': 3849, 'sadness': 3830, 'fear': 2816, 'anger': 1555, 'disgust': 761})\n",
            "literally haven't seen the sun in a week and it's finally coming out!\n",
            "joy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA386VDALKZj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d51464-fa8f-45d9-c1ff-b83a857487c6"
      },
      "source": [
        "# download lexicons\n",
        "!wget https://raw.githubusercontent.com/jumayel06/Tension-Analysis/master/Notebooks/Emotion%20Recognition%20Notebook/lexicons/Ratings_Warriner_et_al.csv\n",
        "!wget https://raw.githubusercontent.com/jumayel06/Tension-Analysis/master/Notebooks/Emotion%20Recognition%20Notebook/lexicons/NRC-emotion-lexicon-wordlevel-v0.92.txt\n",
        "!wget https://raw.githubusercontent.com/jumayel06/Tension-Analysis/master/Notebooks/Emotion%20Recognition%20Notebook/lexicons/nrc_affect_intensity.txt\n",
        "!wget https://raw.githubusercontent.com/jumayel06/Tension-Analysis/master/Notebooks/Emotion%20Recognition%20Notebook/lexicons/NRC-Hashtag-Emotion-Lexicon-v0.2.txt\n",
        "!wget https://raw.githubusercontent.com/jumayel06/Tension-Analysis/master/Notebooks/Emotion%20Recognition%20Notebook/lexicons/BingLiu.txt\n",
        "!wget https://raw.githubusercontent.com/jumayel06/Tension-Analysis/master/Notebooks/Emotion%20Recognition%20Notebook/lexicons/mpqa.txt\n",
        "!wget https://raw.githubusercontent.com/jumayel06/Tension-Analysis/master/Notebooks/Emotion%20Recognition%20Notebook/lexicons/AFINN-en-165.txt\n",
        "!wget https://raw.githubusercontent.com/jumayel06/Tension-Analysis/master/Notebooks/Emotion%20Recognition%20Notebook/lexicons/stopwords.txt\n",
        "!wget https://raw.githubusercontent.com/jumayel06/Tension-Analysis/master/Notebooks/Emotion%20Recognition%20Notebook/lexicons/slangs.txt\n",
        "!wget https://raw.githubusercontent.com/jumayel06/Tension-Analysis/master/Notebooks/Emotion%20Recognition%20Notebook/lexicons/negated_words.txt\n",
        "!wget https://raw.githubusercontent.com/jumayel06/Tension-Analysis/master/Notebooks/Emotion%20Recognition%20Notebook/lexicons/emoticons.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-11 16:50:55--  https://raw.githubusercontent.com/jumayel06/Tension-Analysis/master/Notebooks/Emotion%20Recognition%20Notebook/lexicons/Ratings_Warriner_et_al.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3736801 (3.6M) [text/plain]\n",
            "Saving to: ‘Ratings_Warriner_et_al.csv’\n",
            "\n",
            "Ratings_Warriner_et 100%[===================>]   3.56M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2024-01-11 16:50:55 (40.8 MB/s) - ‘Ratings_Warriner_et_al.csv’ saved [3736801/3736801]\n",
            "\n",
            "--2024-01-11 16:50:55--  https://raw.githubusercontent.com/jumayel06/Tension-Analysis/master/Notebooks/Emotion%20Recognition%20Notebook/lexicons/NRC-emotion-lexicon-wordlevel-v0.92.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 687711 (672K) [text/plain]\n",
            "Saving to: ‘NRC-emotion-lexicon-wordlevel-v0.92.txt’\n",
            "\n",
            "NRC-emotion-lexicon 100%[===================>] 671.59K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-01-11 16:50:56 (11.7 MB/s) - ‘NRC-emotion-lexicon-wordlevel-v0.92.txt’ saved [687711/687711]\n",
            "\n",
            "--2024-01-11 16:50:56--  https://raw.githubusercontent.com/jumayel06/Tension-Analysis/master/Notebooks/Emotion%20Recognition%20Notebook/lexicons/nrc_affect_intensity.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 214884 (210K) [text/plain]\n",
            "Saving to: ‘nrc_affect_intensity.txt’\n",
            "\n",
            "nrc_affect_intensit 100%[===================>] 209.85K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-01-11 16:50:56 (6.28 MB/s) - ‘nrc_affect_intensity.txt’ saved [214884/214884]\n",
            "\n",
            "--2024-01-11 16:50:56--  https://raw.githubusercontent.com/jumayel06/Tension-Analysis/master/Notebooks/Emotion%20Recognition%20Notebook/lexicons/NRC-Hashtag-Emotion-Lexicon-v0.2.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1154680 (1.1M) [text/plain]\n",
            "Saving to: ‘NRC-Hashtag-Emotion-Lexicon-v0.2.txt’\n",
            "\n",
            "NRC-Hashtag-Emotion 100%[===================>]   1.10M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-01-11 16:50:56 (16.4 MB/s) - ‘NRC-Hashtag-Emotion-Lexicon-v0.2.txt’ saved [1154680/1154680]\n",
            "\n",
            "--2024-01-11 16:50:57--  https://raw.githubusercontent.com/jumayel06/Tension-Analysis/master/Notebooks/Emotion%20Recognition%20Notebook/lexicons/BingLiu.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 124954 (122K) [text/plain]\n",
            "Saving to: ‘BingLiu.txt’\n",
            "\n",
            "BingLiu.txt         100%[===================>] 122.03K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-01-11 16:50:57 (4.43 MB/s) - ‘BingLiu.txt’ saved [124954/124954]\n",
            "\n",
            "--2024-01-11 16:50:57--  https://raw.githubusercontent.com/jumayel06/Tension-Analysis/master/Notebooks/Emotion%20Recognition%20Notebook/lexicons/mpqa.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 150171 (147K) [text/plain]\n",
            "Saving to: ‘mpqa.txt’\n",
            "\n",
            "mpqa.txt            100%[===================>] 146.65K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-01-11 16:50:57 (5.12 MB/s) - ‘mpqa.txt’ saved [150171/150171]\n",
            "\n",
            "--2024-01-11 16:50:57--  https://raw.githubusercontent.com/jumayel06/Tension-Analysis/master/Notebooks/Emotion%20Recognition%20Notebook/lexicons/AFINN-en-165.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 39467 (39K) [text/plain]\n",
            "Saving to: ‘AFINN-en-165.txt’\n",
            "\n",
            "AFINN-en-165.txt    100%[===================>]  38.54K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2024-01-11 16:50:57 (7.28 MB/s) - ‘AFINN-en-165.txt’ saved [39467/39467]\n",
            "\n",
            "--2024-01-11 16:50:57--  https://raw.githubusercontent.com/jumayel06/Tension-Analysis/master/Notebooks/Emotion%20Recognition%20Notebook/lexicons/stopwords.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 864 [text/plain]\n",
            "Saving to: ‘stopwords.txt’\n",
            "\n",
            "stopwords.txt       100%[===================>]     864  --.-KB/s    in 0s      \n",
            "\n",
            "2024-01-11 16:50:58 (47.0 MB/s) - ‘stopwords.txt’ saved [864/864]\n",
            "\n",
            "--2024-01-11 16:50:58--  https://raw.githubusercontent.com/jumayel06/Tension-Analysis/master/Notebooks/Emotion%20Recognition%20Notebook/lexicons/slangs.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1783 (1.7K) [text/plain]\n",
            "Saving to: ‘slangs.txt’\n",
            "\n",
            "slangs.txt          100%[===================>]   1.74K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-01-11 16:50:58 (19.8 MB/s) - ‘slangs.txt’ saved [1783/1783]\n",
            "\n",
            "--2024-01-11 16:50:58--  https://raw.githubusercontent.com/jumayel06/Tension-Analysis/master/Notebooks/Emotion%20Recognition%20Notebook/lexicons/negated_words.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 411 [text/plain]\n",
            "Saving to: ‘negated_words.txt’\n",
            "\n",
            "negated_words.txt   100%[===================>]     411  --.-KB/s    in 0s      \n",
            "\n",
            "2024-01-11 16:50:58 (16.4 MB/s) - ‘negated_words.txt’ saved [411/411]\n",
            "\n",
            "--2024-01-11 16:50:58--  https://raw.githubusercontent.com/jumayel06/Tension-Analysis/master/Notebooks/Emotion%20Recognition%20Notebook/lexicons/emoticons.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 292 [text/plain]\n",
            "Saving to: ‘emoticons.txt’\n",
            "\n",
            "emoticons.txt       100%[===================>]     292  --.-KB/s    in 0s      \n",
            "\n",
            "2024-01-11 16:50:58 (3.89 MB/s) - ‘emoticons.txt’ saved [292/292]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTomvBy2LQWY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb37d257-52f9-4282-db15-7f3c3dc83386"
      },
      "source": [
        "# ********* Load Lexicons ********* #\n",
        "bingliu_mpqa = {}\n",
        "nrc_emotion = {}\n",
        "nrc_affect_intensity = {}\n",
        "nrc_hashtag_emotion = {}\n",
        "afinn = {}\n",
        "ratings = {}\n",
        "stopwords = []\n",
        "slangs = {}\n",
        "negated = {}\n",
        "emoticons = []\n",
        "\n",
        "# Vader\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def load_lexicons():\n",
        "    # Ratings by Warriner et al. (2013)\n",
        "    with open('Ratings_Warriner_et_al.csv', 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        rows = list(reader)\n",
        "    print(rows)\n",
        "    for i in range(1, len(rows)):\n",
        "        # Normalize values\n",
        "        valence = (float(rows[i][2]) - 1.0)/(9.0-1.0)\n",
        "        arousal = (float(rows[i][5]) - 1.0)/(9.0-1.0)\n",
        "        dominance = (float(rows[i][8]) - 1.0)/(9.0-1.0)\n",
        "        ratings[rows[i][1]] = {\"Valence\": valence, \"Arousal\": arousal, \"Dominance\": dominance}\n",
        "\n",
        "\n",
        "    # NRC Emotion Lexicon (2014)\n",
        "    with open('NRC-emotion-lexicon-wordlevel-v0.92.txt', 'r') as f:\n",
        "        f.readline()\n",
        "        for line in f:\n",
        "            splitted = line.strip().split('\\t')\n",
        "            if splitted[0] not in nrc_emotion:\n",
        "                nrc_emotion[splitted[0]] = {'anger': float(splitted[1]),\n",
        "                                                    'disgust': float(splitted[3]),\n",
        "                                                    'fear': float(splitted[4]),\n",
        "                                                    'joy': float(splitted[5]),\n",
        "                                                    'sadness': float(splitted[8]),\n",
        "                                                    'surprise': float(splitted[9])}\n",
        "\n",
        "    # NRC Affect Intensity (2018)\n",
        "    with open('nrc_affect_intensity.txt', 'r') as f:\n",
        "        f.readline()\n",
        "        for line in f:\n",
        "            splitted = line.strip().split('\\t')\n",
        "            if splitted[0] not in nrc_affect_intensity:\n",
        "                nrc_affect_intensity[splitted[0]] = {'anger': float(splitted[1]),\n",
        "                                                    'disgust': float(splitted[3]),\n",
        "                                                    'fear': float(splitted[4]),\n",
        "                                                    'joy': float(splitted[5]),\n",
        "                                                    'sadness': float(splitted[8]),\n",
        "                                                    'surprise': float(splitted[9])}\n",
        "\n",
        "    # NRC Hashtag Emotion Lexicon (2015)\n",
        "    with open('NRC-Hashtag-Emotion-Lexicon-v0.2.txt', 'r') as f:\n",
        "        f.readline()\n",
        "        for line in f:\n",
        "            splitted = line.strip().split('\\t')\n",
        "            splitted[0] = splitted[0].replace('#','')\n",
        "            if splitted[0] not in nrc_hashtag_emotion:\n",
        "                nrc_hashtag_emotion[splitted[0]] = {'anger': float(splitted[1]),\n",
        "                                                    'disgust': float(splitted[3]),\n",
        "                                                    'fear': float(splitted[4]),\n",
        "                                                    'joy': float(splitted[5]),\n",
        "                                                    'sadness': float(splitted[8]),\n",
        "                                                    'surprise': float(splitted[9])}\n",
        "\n",
        "\n",
        "    # BingLiu (2004) and MPQA (2005)\n",
        "    with open('BingLiu.txt', 'r') as f:\n",
        "        for line in f:\n",
        "            splitted = line.strip().split('\\t')\n",
        "            if splitted[0] not in bingliu_mpqa:\n",
        "                bingliu_mpqa[splitted[0]] = splitted[1]\n",
        "    with open('mpqa.txt', 'r') as f:\n",
        "        for line in f:\n",
        "            splitted = line.strip().split('\\t')\n",
        "            if splitted[0] not in bingliu_mpqa:\n",
        "                bingliu_mpqa[splitted[0]] = splitted[1]\n",
        "\n",
        "\n",
        "    with open('AFINN-en-165.txt', 'r') as f:\n",
        "        for line in f:\n",
        "            splitted = line.strip().split('\\t')\n",
        "            if splitted[0] not in afinn:\n",
        "                score = float(splitted[1])\n",
        "                normalized_score = (score - (-5)) / (5-(-5))\n",
        "                afinn[splitted[0]] = normalized_score\n",
        "\n",
        "\n",
        "    with open('stopwords.txt', 'r') as f:\n",
        "        for line in f:\n",
        "            stopwords.append(line.strip())\n",
        "\n",
        "    with open('slangs.txt', 'r') as f:\n",
        "        for line in f:\n",
        "            splitted = line.strip().split(',', 1)\n",
        "            slangs[splitted[0]] = splitted[1]\n",
        "\n",
        "    with open('negated_words.txt', 'r') as f:\n",
        "        for line in f:\n",
        "            splitted = line.strip().split(',', 1)\n",
        "            negated[splitted[0]] = splitted[1]\n",
        "\n",
        "    with open('emoticons.txt', 'r') as f:\n",
        "        for line in f:\n",
        "            emoticons.append(line.strip())\n",
        "load_lexicons()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtvYxFCnN0fV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc6f86cd-c679-41f6-c436-ddf5c3615fad"
      },
      "source": [
        "# Now grab the GloVe embeddings we will need: Takes about a minute to download.\n",
        "# Then it take about another minute to fill the data structure.\n",
        "# Note that the zip file with the embeddings is hosted in Dropbox. If this does\n",
        "# not work, it could be downloaded from the GloVe website and uploaded to the\n",
        "# file store for this notebook.\n",
        "\n",
        "#!wget https://www.dropbox.com/s/ewfdwppopt3pild/glove.twitter.27B.100d.txt.zip?dl=1\n",
        "!wget  https://www.dropbox.com/s/tq8t97grqd7vjxc/glove.twitter.27B.100d.txt.zip?dl=0\n",
        "!unzip glove.twitter.27B.100d.txt.zip?dl=0\n",
        "print(\"Loading word embeddings...\")\n",
        "embeddings_index = dict() # Initialize an empty dictionary\n",
        "embedding_dir = 'glove.twitter.27B.100d.txt'\n",
        "\n",
        "f = open(embedding_dir,encoding=\"utf8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-11 16:50:59--  https://www.dropbox.com/s/tq8t97grqd7vjxc/glove.twitter.27B.100d.txt.zip?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.80.18, 2620:100:601f:18::a27d:912\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.80.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/tq8t97grqd7vjxc/glove.twitter.27B.100d.txt.zip [following]\n",
            "--2024-01-11 16:51:00--  https://www.dropbox.com/s/raw/tq8t97grqd7vjxc/glove.twitter.27B.100d.txt.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucdaa58deb2ffe6b7e879e9d7657.dl.dropboxusercontent.com/cd/0/inline/CLIk2TKN1giqVGeaDGm5-z1Kfs1rrc1ww3V4D5Qhpf2XFdx6OOsnyAoaYedzkve5GzZ7GWsjdNcGrc0HDFM1qUWnuC-J5rAnInyw7vkdyX8Sw9z1K-rnD1a28ADi6IScUirai-SdFyKtlADWj3ck-QFt/file# [following]\n",
            "--2024-01-11 16:51:01--  https://ucdaa58deb2ffe6b7e879e9d7657.dl.dropboxusercontent.com/cd/0/inline/CLIk2TKN1giqVGeaDGm5-z1Kfs1rrc1ww3V4D5Qhpf2XFdx6OOsnyAoaYedzkve5GzZ7GWsjdNcGrc0HDFM1qUWnuC-J5rAnInyw7vkdyX8Sw9z1K-rnD1a28ADi6IScUirai-SdFyKtlADWj3ck-QFt/file\n",
            "Resolving ucdaa58deb2ffe6b7e879e9d7657.dl.dropboxusercontent.com (ucdaa58deb2ffe6b7e879e9d7657.dl.dropboxusercontent.com)... 162.125.80.15, 2620:100:601f:15::a27d:90f\n",
            "Connecting to ucdaa58deb2ffe6b7e879e9d7657.dl.dropboxusercontent.com (ucdaa58deb2ffe6b7e879e9d7657.dl.dropboxusercontent.com)|162.125.80.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CLJx_bEXJU6oqk0bM_f6Cw5tEuc5DP41AHOGPbpfC1M1EbHTl6G1OHo4vlE_zKdziYHeiUnYK3J6Pny9wSPRiFk966w7lgMIkJQe9NEMfpKvKCPGlOzXeE1CM9M6rywAU3SU2dxBq4rfIKXWOS6TgZn9aaI0ic07nC_LZgZh7odiWnOTMzmWnpzwjtsqqGWi6VYEM-g2CNXdqsCU3qFeOOLxkjQ1AxHUGYWXJfhGfwzoKUdic3NevYY2XhvrLAmLVcvFRL-WxQvCiHrG0JVU1MtVGjBs4AyasjmsiTcazMyeCzdIOwedOroBO3TioBmoJimAvrx-IxfbJd02c12Nyk0gPDp_G1HViXsPjkJTXa-7y-_VYzhJDPdjnoPjqkgRCsQ/file [following]\n",
            "--2024-01-11 16:51:02--  https://ucdaa58deb2ffe6b7e879e9d7657.dl.dropboxusercontent.com/cd/0/inline2/CLJx_bEXJU6oqk0bM_f6Cw5tEuc5DP41AHOGPbpfC1M1EbHTl6G1OHo4vlE_zKdziYHeiUnYK3J6Pny9wSPRiFk966w7lgMIkJQe9NEMfpKvKCPGlOzXeE1CM9M6rywAU3SU2dxBq4rfIKXWOS6TgZn9aaI0ic07nC_LZgZh7odiWnOTMzmWnpzwjtsqqGWi6VYEM-g2CNXdqsCU3qFeOOLxkjQ1AxHUGYWXJfhGfwzoKUdic3NevYY2XhvrLAmLVcvFRL-WxQvCiHrG0JVU1MtVGjBs4AyasjmsiTcazMyeCzdIOwedOroBO3TioBmoJimAvrx-IxfbJd02c12Nyk0gPDp_G1HViXsPjkJTXa-7y-_VYzhJDPdjnoPjqkgRCsQ/file\n",
            "Reusing existing connection to ucdaa58deb2ffe6b7e879e9d7657.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 406221610 (387M) [application/zip]\n",
            "Saving to: ‘glove.twitter.27B.100d.txt.zip?dl=0’\n",
            "\n",
            "glove.twitter.27B.1 100%[===================>] 387.40M  10.8MB/s    in 33s     \n",
            "\n",
            "2024-01-11 16:51:35 (11.9 MB/s) - ‘glove.twitter.27B.100d.txt.zip?dl=0’ saved [406221610/406221610]\n",
            "\n",
            "Archive:  glove.twitter.27B.100d.txt.zip?dl=0\n",
            "  inflating: glove.twitter.27B.100d.txt  \n",
            "  inflating: __MACOSX/._glove.twitter.27B.100d.txt  \n",
            "Loading word embeddings...\n",
            "Loaded 1193514 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpBZc5RBNc1I"
      },
      "source": [
        "# ********* Hyper-parameters configurations ********* #\n",
        "\n",
        "# Fix your seed\n",
        "seed = 66\n",
        "np.random.seed(seed)\n",
        "\n",
        "# List of emotions you are going to use in ascending order\n",
        "emotion_categories = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
        "num_categories = len(emotion_categories)\n",
        "\n",
        "# Word and Hash-emo embedding dimension\n",
        "dimension = 100\n",
        "\n",
        "# Lexical feature dimension\n",
        "feature_dimension = 29\n",
        "\n",
        "filters = [128, 128, 128, 128]\n",
        "# prevent overfitting\n",
        "dropout_rates = [0.5, 0.5, 0.5, 0.5]\n",
        "# imitate n-gram\n",
        "kernel_sizes = [1, 2, 3, 1]\n",
        "hidden = [200, 100, 10]\n",
        "\n",
        "epochs = 5\n",
        "batch_size = 64\n",
        "\n",
        "embedding_dir = 'glove.twitter.27B.100d.txt'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V848PTNGRxH9"
      },
      "source": [
        "# ********* Helper Functions ********* #\n",
        "def char_is_emoji(character):\n",
        "    #return character in emoji.UNICODE_EMOJI\n",
        "    return emoji.is_emoji(character[0])  #check out the API for emoji here: https://carpedm20.github.io/emoji/docs/api.html#\n",
        "\n",
        "def text_has_emoji(text):\n",
        "    for character in text:\n",
        "        if char_is_emoji(character):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def clean_tweets(texts):\n",
        "    cleaned_tweets = []\n",
        "    hash_emos = []\n",
        "\n",
        "    for text in texts:\n",
        "        hash_emo = []\n",
        "        text = re.sub('(!){2,}', ' <!repeat> ', text)\n",
        "        text = re.sub('(\\?){2,}', ' <?repeat> ', text)\n",
        "\n",
        "        # Tokenize using tweet tokenizer\n",
        "        tokenizer = TweetTokenizer(strip_handles=False, reduce_len=True)\n",
        "        tokens = tokenizer.tokenize(text.lower())\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "        # Emojis and emoticons\n",
        "        if text_has_emoji(text):\n",
        "            temp = []\n",
        "            for word in tokens:\n",
        "                if char_is_emoji(word):\n",
        "                    hash_emo.append(emoji.demojize(word))\n",
        "                elif word in emoticons:\n",
        "                    hash_emo.append(word)\n",
        "                else:\n",
        "                    temp.append(word)\n",
        "            tokens = temp\n",
        "\n",
        "        # Hashtags\n",
        "        temp = []\n",
        "        for word in tokens:\n",
        "            if '#' in word:\n",
        "                word = word.replace('#','')\n",
        "                hash_emo.append(word)\n",
        "            else:\n",
        "                temp.append(word)\n",
        "        tokens = temp\n",
        "\n",
        "        # Replace slangs and negated words\n",
        "        temp = []\n",
        "        for word in tokens:\n",
        "            if word in slangs:\n",
        "                temp += slangs[word].split()\n",
        "            elif word in negated:\n",
        "                temp += negated[word].split()\n",
        "            else:\n",
        "                temp.append(word)\n",
        "        tokens = temp\n",
        "\n",
        "        # Replace user names\n",
        "        tokens = ['<user>'  if '@' in word else word for word in tokens]\n",
        "\n",
        "        #Replace numbers\n",
        "        tokens = ['<number>' if word.isdigit() else word for word in tokens]\n",
        "\n",
        "        # Remove urls\n",
        "        tokens = ['' if 'http' in word else word for word in tokens]\n",
        "\n",
        "        # Lemmatize\n",
        "        #tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "        # Remove stop words\n",
        "        tokens = [word for word in tokens if word not in stopwords]\n",
        "\n",
        "        # Remove tokens having length 1\n",
        "        tokens = [word for word in tokens if word != '' and len(word) > 1]\n",
        "\n",
        "        cleaned_tweets.append(tokens)\n",
        "        hash_emos.append(hash_emo)\n",
        "\n",
        "    return cleaned_tweets, hash_emos"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "detMGkdjbKxA"
      },
      "source": [
        "# This function returns a n-dimensional feature vector\n",
        "def feature_generation(texts, hashtags):\n",
        "    feature_vectors = []\n",
        "\n",
        "    for i in range(len(texts)):\n",
        "        feats = [0] * feature_dimension\n",
        "        for word in texts[i]:\n",
        "            # Warriner er al.\n",
        "            if word in ratings:\n",
        "                feats[0] += ratings[word]['Valence']\n",
        "                feats[1] += ratings[word]['Arousal']\n",
        "                feats[2] += ratings[word]['Dominance']\n",
        "\n",
        "            # Vader Sentiment\n",
        "            polarity_scores = analyzer.polarity_scores(word)\n",
        "            feats[3] += polarity_scores['pos']\n",
        "            feats[4] += polarity_scores['neg']\n",
        "            feats[5] += polarity_scores['neu']\n",
        "\n",
        "            # NRC Emotion\n",
        "            if word in nrc_emotion:\n",
        "                feats[6] += nrc_emotion[word]['anger']\n",
        "                feats[7] += nrc_emotion[word]['disgust']\n",
        "                feats[8] += nrc_emotion[word]['fear']\n",
        "                feats[9] += nrc_emotion[word]['joy']\n",
        "                feats[10] += nrc_emotion[word]['sadness']\n",
        "                feats[11] += nrc_emotion[word]['surprise']\n",
        "\n",
        "            # NRC Affect Intensity\n",
        "            if word in nrc_affect_intensity:\n",
        "                feats[12] += nrc_affect_intensity[word]['anger']\n",
        "                feats[13] += nrc_affect_intensity[word]['disgust']\n",
        "                feats[14] += nrc_affect_intensity[word]['fear']\n",
        "                feats[15] += nrc_affect_intensity[word]['joy']\n",
        "                feats[16] += nrc_affect_intensity[word]['sadness']\n",
        "                feats[17] += nrc_affect_intensity[word]['surprise']\n",
        "\n",
        "            # AFINN\n",
        "            if word in afinn:\n",
        "                feats[18] += float(afinn[word])\n",
        "\n",
        "            # BingLiu and MPQA\n",
        "            if word in bingliu_mpqa:\n",
        "                if bingliu_mpqa[word] == 'positive':\n",
        "                    feats[19] += 1\n",
        "                else:\n",
        "                    feats[20] += 1\n",
        "\n",
        "\n",
        "        count = len(texts[i])\n",
        "        if count == 0:\n",
        "            count = 1\n",
        "        newArray = np.array(feats)/count\n",
        "        feats = list(newArray)\n",
        "\n",
        "        # Presence of consecutive exclamation mark or question mark\n",
        "        for word in texts[i]:\n",
        "            if word == '<!REPEAT>':\n",
        "                feats[21] = 1\n",
        "            elif word == '<?REPEAT>':\n",
        "                feats[22] = 1\n",
        "\n",
        "        for word in hashtags[i]:\n",
        "            #NRC Hashtag Emotion\n",
        "            if word in nrc_hashtag_emotion:\n",
        "                feats[23] += nrc_hashtag_emotion[word]['anger']\n",
        "                feats[24] += nrc_hashtag_emotion[word]['disgust']\n",
        "                feats[25] += nrc_hashtag_emotion[word]['fear']\n",
        "                feats[26] += nrc_hashtag_emotion[word]['joy']\n",
        "                feats[27] += nrc_hashtag_emotion[word]['sadness']\n",
        "                feats[28] += nrc_hashtag_emotion[word]['surprise']\n",
        "\n",
        "        feature_vectors.append(feats)\n",
        "    return np.array(feature_vectors)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-2HmfCFbNzd"
      },
      "source": [
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines) #https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do\n",
        "    return tokenizer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFXP47awbQUZ"
      },
      "source": [
        "def max_length(lines):\n",
        "    return max([len(s) for s in lines])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LaHDILUbSKq"
      },
      "source": [
        "def encode_text(tokenizer, lines, length):\n",
        "    encoded = tokenizer.texts_to_sequences(lines) #https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
        "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        "    return padded"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizertest = Tokenizer()\n",
        "tokenizertest.fit_on_texts([\"this is a great day\"])\n",
        "sequences = tokenizertest.texts_to_sequences([\"this is a day\", \"this is a good day\"])\n",
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIP0gWbrYpml",
        "outputId": "cbb55eb6-c20c-4300-ce22-d07193d9d78e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 3, 5], [1, 2, 3, 5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMnCM690bXLb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9700db63-aee6-4e5e-f0d5-9a030c98d6f7"
      },
      "source": [
        "print(\"Cleaning Data...\")\n",
        "cleaned_tweets, hash_emos = clean_tweets(texts)\n",
        "print(\"Cleaning Completed!\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning Data...\n",
            "Cleaning Completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNtdkfTebcsQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91b2fe8a-3684-47ff-c807-8df9e3c8b2bd"
      },
      "source": [
        "print(\"Generating Features...\")\n",
        "features = feature_generation(cleaned_tweets, hash_emos)\n",
        "print(\"Feature Generation Completed!\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating Features...\n",
            "Feature Generation Completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7tS2WIobfhh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa7f98cc-2844-4749-b551-645b292acabf"
      },
      "source": [
        "print(\"Encoding Data...\")\n",
        "# For Tweet Matrix\n",
        "tokenizer_tweets = create_tokenizer(cleaned_tweets)\n",
        "max_tweet_length = max_length(cleaned_tweets)\n",
        "vocab_size = len(tokenizer_tweets.word_index) + 1\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "X = encode_text(tokenizer_tweets, cleaned_tweets, max_tweet_length)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding Data...\n",
            "Vocabulary size: 24671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ah5bNJ6bh-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9979575-07c2-478d-da25-81a5db82d956"
      },
      "source": [
        "# For Hash-Emo Matrix\n",
        "tokenizer_hash_emo = create_tokenizer(hash_emos)\n",
        "max_hash_emo_length = max_length(hash_emos)\n",
        "vocab_size_hash_emo = len(tokenizer_hash_emo.word_index) + 1\n",
        "print('Vocabulary size (Hash-Emos): %d' % vocab_size_hash_emo)\n",
        "encoded_hash_emo = encode_text(tokenizer_hash_emo, hash_emos, max_hash_emo_length)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size (Hash-Emos): 3533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWXl8I6iblEP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dc9f170-72df-4c7c-f3de-81987d881cca"
      },
      "source": [
        "# Labels\n",
        "lb = LabelBinarizer() #https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html\n",
        "lb.fit(labels)\n",
        "Y = lb.transform(labels)\n",
        "print(\"Encoding Completed!\")\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding Completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdqP-86Ebm-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcdf9848-b672-4a35-d846-4ea7f5b04d59"
      },
      "source": [
        "# Load embedding\n",
        "print(\"Loading word embeddings...\")\n",
        "embeddings_index = dict()\n",
        "f = open(embedding_dir, encoding=\"utf8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading word embeddings...\n",
            "Loaded 1193514 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dHUnMUbbqqo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429e1806-a526-4fc5-d98f-7e4dfc0db0b8"
      },
      "source": [
        "# Generate embedding matrices\n",
        "print(\"Generating embedding matrices...\")\n",
        "tweet_matrix = zeros((vocab_size, dimension))\n",
        "for word, i in tokenizer_tweets.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        tweet_matrix[i] = np.array(list(embedding_vector))\n",
        "    else:\n",
        "        tweet_matrix[i] = np.array(list(np.random.uniform(low=-1, high=1, size=(100,))))\n",
        "tweet_matrix[1]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating embedding matrices...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.63006002,  0.65177   ,  0.25545001,  0.018593  ,  0.043094  ,\n",
              "        0.047194  ,  0.23218   ,  0.11613   ,  0.17371   ,  0.40487   ,\n",
              "        0.022524  , -0.076731  , -2.29110003,  0.094127  ,  0.43292999,\n",
              "        0.041801  ,  0.063175  , -0.64486003, -0.43656999,  0.024114  ,\n",
              "       -0.082989  ,  0.21686   , -0.13462   , -0.22336   ,  0.39436001,\n",
              "       -2.1724    , -0.39544001,  0.16536   ,  0.39438   , -0.35181999,\n",
              "       -0.14996   ,  0.10502   , -0.45936999,  0.27728999,  0.89240003,\n",
              "       -0.042313  , -0.009345  ,  0.55017   ,  0.095521  ,  0.070504  ,\n",
              "       -1.17809999,  0.013723  ,  0.17742001,  0.74141997,  0.17715999,\n",
              "        0.038468  , -0.31683999,  0.08941   ,  0.20557   , -0.34327999,\n",
              "       -0.64302999, -0.87800002, -0.16293   , -0.055925  ,  0.33897999,\n",
              "        0.60663998, -0.27739999,  0.33625999,  0.21603   , -0.11051   ,\n",
              "        0.0058673 , -0.64757001, -0.068222  , -0.77414   ,  0.13911   ,\n",
              "       -0.15851   , -0.61884999, -0.10192   , -0.47      ,  0.19787   ,\n",
              "        0.42175001, -0.18458   ,  0.080581  , -0.22544999, -0.065129  ,\n",
              "       -0.15328   ,  0.087726  , -0.18817   , -0.08371   ,  0.21778999,\n",
              "        0.97899002,  0.1092    ,  0.022705  , -0.078234  ,  0.15594999,\n",
              "        0.083105  , -0.68239999,  0.57468998, -0.19942001,  0.50566   ,\n",
              "       -0.18277   ,  0.37720999, -0.12514   , -0.42820999, -0.81075001,\n",
              "       -0.39326   , -0.17386   ,  0.55096   ,  0.64705998, -0.60930002])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpU-rS4Cbu_p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d07476d-e2c4-4c6e-f815-e32434a96fe8"
      },
      "source": [
        "hash_emo_matrix = zeros((vocab_size_hash_emo, dimension))\n",
        "\n",
        "for word, i in tokenizer_hash_emo.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        hash_emo_matrix[i] = np.array(list(embedding_vector))\n",
        "    else:\n",
        "        hash_emo_matrix[i] = np.array(list(np.random.uniform(low=-1, high=1, size=(100,))))\n",
        "hash_emo_matrix[1]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-4.60200012e-01, -1.83950007e-01,  1.37390003e-01,  2.15160009e-02,\n",
              "       -1.22000001e-01,  2.93669999e-01,  3.08589995e-01,  3.32630007e-03,\n",
              "       -8.08589995e-01,  4.45800006e-01, -1.38600007e-01, -8.68040025e-01,\n",
              "       -4.18139982e+00, -1.53659999e-01, -4.63270009e-01,  4.71199989e-01,\n",
              "       -2.27579996e-01, -2.09279999e-01,  3.24140012e-01,  3.86680007e-01,\n",
              "        2.10630000e-01, -2.42760003e-01, -1.28959998e-01,  3.17400008e-01,\n",
              "       -1.60490006e-01,  6.44840002e-01, -4.66430008e-01,  9.82540026e-02,\n",
              "        5.34170009e-02,  1.77709997e-01,  1.75960004e-01, -1.85690001e-01,\n",
              "       -7.76479989e-02,  9.86459970e-01,  8.71779956e-03, -1.73179999e-01,\n",
              "        6.41369969e-02,  1.77169994e-01,  1.39709994e-01,  1.64010003e-01,\n",
              "       -1.04130006e+00, -2.06540003e-01, -4.04619984e-02,  1.92770008e-02,\n",
              "        2.62919992e-01, -3.42739999e-01, -3.44020009e-01,  5.51840007e-01,\n",
              "       -1.03840005e+00,  1.40499997e+00, -1.71939999e-01,  2.04270005e-01,\n",
              "       -2.39350006e-01,  4.61870015e-01, -2.11190000e-01,  9.09640014e-01,\n",
              "        1.24920003e-01,  7.06129968e-02,  4.96449992e-02,  1.42140001e-01,\n",
              "        2.45829999e-01,  1.42550007e-01,  5.35479980e-03,  2.58990005e-02,\n",
              "        8.86919975e-01,  3.16900015e-01,  8.52359980e-02,  1.70190006e-01,\n",
              "       -2.32649997e-01, -1.62990004e-01, -2.77980000e-01,  4.31340002e-02,\n",
              "       -7.43509978e-02,  5.18630028e-01,  3.35110009e-01, -3.26709996e-05,\n",
              "       -3.78140002e-01, -4.87879992e-01, -4.02889997e-01, -4.85229999e-01,\n",
              "        9.73490000e-01, -2.84850001e-01, -9.23760012e-02,  3.11040003e-02,\n",
              "       -9.28919986e-02, -1.01750001e-01,  3.39089990e-01, -2.98530012e-01,\n",
              "       -2.36880004e-01,  8.89720023e-02, -6.59110010e-01,  4.75439996e-01,\n",
              "        1.75180003e-01,  5.75699985e-01,  5.72730005e-01,  8.20960030e-02,\n",
              "       -2.18490005e-01, -1.12070002e-01,  3.97280008e-01, -5.06489992e-01])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyom_xMVb-U5"
      },
      "source": [
        "# ********* Model ********* #\n",
        "\n",
        "\n",
        "class TestCallback(Callback):\n",
        "    def __init__(self, test_data):\n",
        "        self.test_data = test_data\n",
        "        self.accs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        x, y = self.test_data\n",
        "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
        "        self.accs.append(acc)\n",
        "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))\n",
        "\n",
        "def model(max_tweet_length, max_hash_emo_length, vocab_size, vocab_size_hash_emo, tweet_matrix, hash_emo_matrix, dimension, feature_dimension, num_categories, train_embedding):\n",
        "\n",
        "    # Channel 1 - tweet text\n",
        "    inputs1 = Input(shape=(max_tweet_length,)) #Input() is used to instantiate a Keras tensor. https://keras.io/api/layers/core_layers/input/\n",
        "    embedding1 = Embedding(vocab_size, dimension, weights=[tweet_matrix], trainable=train_embedding)(inputs1) # Embedding() is used as the first layer https://keras.io/api/layers/core_layers/embedding/#embedding-class\n",
        "\n",
        "    # unigram\n",
        "    conv1 = Conv1D(filters=filters[0], kernel_size=kernel_sizes[0], activation='relu')(embedding1)\n",
        "    drop1 = Dropout(dropout_rates[0])(conv1) #The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting.\n",
        "    pool1 = GlobalMaxPooling1D()(drop1)\n",
        "\n",
        "    # bigram\n",
        "    conv2 = Conv1D(filters=filters[1], kernel_size=kernel_sizes[1], activation='relu')(embedding1)\n",
        "    drop2 = Dropout(dropout_rates[1])(conv2)\n",
        "    pool2 = GlobalMaxPooling1D()(drop2)\n",
        "\n",
        "    #trigram\n",
        "    conv3 = Conv1D(filters=filters[2], kernel_size=kernel_sizes[2], activation='relu')(embedding1)\n",
        "    drop3 = Dropout(dropout_rates[2])(conv3)\n",
        "    pool3 = GlobalMaxPooling1D()(drop3)\n",
        "\n",
        "    # Channel 2 - hashtag, emoji, and emoticon\n",
        "    inputs2 = Input(shape=(max_hash_emo_length,))\n",
        "    embedding2 = Embedding(vocab_size_hash_emo, dimension, weights=[hash_emo_matrix], trainable=train_embedding)(inputs2)\n",
        "    conv4 = Conv1D(filters=filters[3], kernel_size=kernel_sizes[3], activation='relu')(embedding2)\n",
        "    drop4 = Dropout(dropout_rates[3])(conv4)\n",
        "    pool4 = GlobalMaxPooling1D()(drop4)\n",
        "\n",
        "    # Lexical features\n",
        "    features = Input(shape=(feature_dimension,))\n",
        "\n",
        "    #It takes as input a list of tensors and returns a single tensor that is the concatenation of all inputs. https://keras.io/api/layers/merging_layers/concatenate/\n",
        "    merged = concatenate([pool1, pool2, pool3, pool4, features])\n",
        "\n",
        "    dense1 = Dense(hidden[0], activation='relu')(merged) #https://keras.io/api/layers/core_layers/dense/\n",
        "    dense2 = Dense(hidden[1], activation='relu')(dense1)\n",
        "    dense3 = Dense(hidden[2], activation='relu')(dense2)\n",
        "    outputs = Dense(num_categories, activation='softmax')(dense3)\n",
        "\n",
        "    model = Model(inputs=[inputs1, inputs2, features], outputs=outputs)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    #print(model.summary())\n",
        "    #plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFlQoKUIcDaH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5658633d-d0a5-4076-937d-8dab7484fb00"
      },
      "source": [
        "kf = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "kf.get_n_splits(len(labels))\n",
        "\n",
        "accuracies = []\n",
        "counter = 1\n",
        "#for train, test in kf:\n",
        "for train, test in kf.split((labels)):\n",
        "    print('Fold#', counter)\n",
        "    counter += 1\n",
        "    model_GloVe = model(max_tweet_length,\n",
        "                       max_hash_emo_length,\n",
        "                       vocab_size,\n",
        "                       vocab_size_hash_emo,\n",
        "                       tweet_matrix,\n",
        "                       hash_emo_matrix,\n",
        "                       dimension,\n",
        "                       feature_dimension,\n",
        "                       num_categories,\n",
        "                       True)\n",
        "    testObj = TestCallback(([X[test], encoded_hash_emo[test], features[test]], Y[test])) ### print perfromance for each epoch\n",
        "\n",
        "    #earlystop = EarlyStopping(monitor='val_acc', min_delta=0.0001, patience=3, verbose=1, mode='auto')\n",
        "    model_GloVe.fit([X[train], encoded_hash_emo[train], features[train]],\n",
        "                    array(Y[train]),\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    callbacks=[testObj],\n",
        "                    verbose = 1)\n",
        "    scores = model_GloVe.evaluate([X[test], encoded_hash_emo[test], features[test]], Y[test], verbose=0)\n",
        "    print(\"%s: %.2f%%\" % (model_GloVe.metrics_names[1], scores[1]*100)) ### print last epoch's performance\n",
        "    index, value = max(enumerate(testObj.accs), key=operator.itemgetter(1))\n",
        "    accuracies.append(value)\n",
        "    break\n",
        "\n",
        "print(accuracies)\n",
        "print(np.mean(accuracies))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold# 1\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 29)]                 0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 13)]                 0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 29, 100)              2467100   ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, 13, 100)              353300    ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)             (None, 29, 128)              12928     ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)           (None, 28, 128)              25728     ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)           (None, 27, 128)              38528     ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)           (None, 13, 128)              12928     ['embedding_1[0][0]']         \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 29, 128)              0         ['conv1d[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 28, 128)              0         ['conv1d_1[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 27, 128)              0         ['conv1d_2[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)         (None, 13, 128)              0         ['conv1d_3[0][0]']            \n",
            "                                                                                                  \n",
            " global_max_pooling1d (Glob  (None, 128)                  0         ['dropout[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_1 (Gl  (None, 128)                  0         ['dropout_1[0][0]']           \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " global_max_pooling1d_2 (Gl  (None, 128)                  0         ['dropout_2[0][0]']           \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " global_max_pooling1d_3 (Gl  (None, 128)                  0         ['dropout_3[0][0]']           \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)        [(None, 29)]                 0         []                            \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 541)                  0         ['global_max_pooling1d[0][0]',\n",
            "                                                                     'global_max_pooling1d_1[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'global_max_pooling1d_2[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'global_max_pooling1d_3[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'input_3[0][0]']             \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 200)                  108400    ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 100)                  20100     ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 10)                   1010      ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 6)                    66        ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3040088 (11.60 MB)\n",
            "Trainable params: 3040088 (11.60 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5\n",
            "296/297 [============================>.] - ETA: 0s - loss: 1.3274 - accuracy: 0.4784\n",
            "Testing loss: 1.300348162651062, acc: 0.5346628427505493\n",
            "\n",
            "297/297 [==============================] - 28s 85ms/step - loss: 1.3275 - accuracy: 0.4783\n",
            "Epoch 2/5\n",
            "296/297 [============================>.] - ETA: 0s - loss: 1.1499 - accuracy: 0.5560\n",
            "Testing loss: 1.1841669082641602, acc: 0.5588793754577637\n",
            "\n",
            "297/297 [==============================] - 24s 82ms/step - loss: 1.1499 - accuracy: 0.5560\n",
            "Epoch 3/5\n",
            "296/297 [============================>.] - ETA: 0s - loss: 1.0171 - accuracy: 0.6131\n",
            "Testing loss: 1.1333367824554443, acc: 0.5897436141967773\n",
            "\n",
            "297/297 [==============================] - 24s 82ms/step - loss: 1.0170 - accuracy: 0.6131\n",
            "Epoch 4/5\n",
            "296/297 [============================>.] - ETA: 0s - loss: 0.8632 - accuracy: 0.6835\n",
            "Testing loss: 1.1800955533981323, acc: 0.5607787370681763\n",
            "\n",
            "297/297 [==============================] - 22s 75ms/step - loss: 0.8633 - accuracy: 0.6835\n",
            "Epoch 5/5\n",
            "297/297 [==============================] - ETA: 0s - loss: 0.7329 - accuracy: 0.7397\n",
            "Testing loss: 1.1811652183532715, acc: 0.5707502365112305\n",
            "\n",
            "297/297 [==============================] - 25s 85ms/step - loss: 0.7329 - accuracy: 0.7397\n",
            "accuracy: 57.08%\n",
            "[0.5897436141967773]\n",
            "0.5897436141967773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfQuPs2tcJXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deb28f83-fd43-496d-bf23-543a27f9e01f"
      },
      "source": [
        "###### to use this model to identify emotions ######\n",
        "predictiontexts = []\n",
        "!wget https://www.dropbox.com/s/jcq50uf9r4xqbzu/TestTweet.txt\n",
        "\n",
        "with open('TestTweet.txt', 'r', encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        splitted = line.strip().split()\n",
        "        predictiontexts.append(' '.join(splitted[1:len(splitted)-2]))\n",
        "\n",
        "cleaned_test_tweets, hash_test_emos = clean_tweets(predictiontexts)\n",
        "\n",
        "test_features = feature_generation(cleaned_test_tweets, hash_test_emos)\n",
        "test_X = encode_text(tokenizer_tweets, cleaned_test_tweets, max_tweet_length)\n",
        "test_encoded_hash_emo = encode_text(tokenizer_hash_emo, hash_test_emos, max_hash_emo_length)\n",
        "results = model_GloVe.predict([test_X, test_encoded_hash_emo, test_features])\n",
        "predicted_label = [np.argmax(r) for r in results]\n",
        "print(predicted_label)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-11 16:55:40--  https://www.dropbox.com/s/jcq50uf9r4xqbzu/TestTweet.txt\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.80.18, 2620:100:601f:18::a27d:912\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.80.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/jcq50uf9r4xqbzu/TestTweet.txt [following]\n",
            "--2024-01-11 16:55:41--  https://www.dropbox.com/s/raw/jcq50uf9r4xqbzu/TestTweet.txt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uca1f5c4c526c022870aa0b7ef9f.dl.dropboxusercontent.com/cd/0/inline/CLK_hwSWumgwvUCulQVhAROv2RHTlhcKOKci00KzL7E0dZNT-fZU8jv0zkR9WMB0A5rWtN8f3SOxm3euRKK-_tD7yBaZGKOze7zHiEAUy4f1mMNm6thWXVhYf7HKE-U5W-uMVd95TpbCUMC0noKi8Hue/file# [following]\n",
            "--2024-01-11 16:55:41--  https://uca1f5c4c526c022870aa0b7ef9f.dl.dropboxusercontent.com/cd/0/inline/CLK_hwSWumgwvUCulQVhAROv2RHTlhcKOKci00KzL7E0dZNT-fZU8jv0zkR9WMB0A5rWtN8f3SOxm3euRKK-_tD7yBaZGKOze7zHiEAUy4f1mMNm6thWXVhYf7HKE-U5W-uMVd95TpbCUMC0noKi8Hue/file\n",
            "Resolving uca1f5c4c526c022870aa0b7ef9f.dl.dropboxusercontent.com (uca1f5c4c526c022870aa0b7ef9f.dl.dropboxusercontent.com)... 162.125.13.15, 2620:100:601f:15::a27d:90f\n",
            "Connecting to uca1f5c4c526c022870aa0b7ef9f.dl.dropboxusercontent.com (uca1f5c4c526c022870aa0b7ef9f.dl.dropboxusercontent.com)|162.125.13.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 45357 (44K) [text/plain]\n",
            "Saving to: ‘TestTweet.txt’\n",
            "\n",
            "TestTweet.txt       100%[===================>]  44.29K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-01-11 16:55:42 (438 KB/s) - ‘TestTweet.txt’ saved [45357/45357]\n",
            "\n",
            "13/13 [==============================] - 0s 6ms/step\n",
            "[5, 4, 4, 3, 0, 2, 3, 3, 2, 0, 4, 5, 0, 4, 3, 0, 3, 3, 3, 0, 2, 3, 3, 3, 0, 3, 3, 2, 4, 3, 5, 0, 3, 4, 3, 3, 3, 4, 0, 0, 2, 5, 1, 0, 3, 4, 5, 2, 3, 2, 4, 2, 0, 3, 3, 3, 3, 3, 3, 4, 2, 3, 3, 3, 4, 3, 3, 0, 3, 3, 3, 3, 4, 3, 3, 3, 4, 3, 3, 2, 3, 4, 5, 3, 3, 2, 3, 4, 5, 3, 5, 0, 2, 3, 3, 3, 5, 0, 2, 3, 3, 3, 0, 3, 2, 3, 5, 3, 2, 3, 5, 3, 3, 0, 3, 3, 4, 4, 3, 5, 5, 3, 3, 4, 0, 3, 4, 4, 2, 0, 0, 3, 3, 5, 1, 5, 5, 0, 2, 3, 5, 0, 3, 0, 3, 3, 3, 3, 5, 3, 3, 4, 3, 5, 3, 3, 3, 0, 3, 0, 3, 5, 3, 3, 2, 3, 3, 3, 3, 5, 3, 5, 2, 0, 0, 0, 4, 5, 3, 2, 2, 3, 3, 3, 0, 5, 3, 3, 0, 5, 2, 5, 0, 0, 3, 4, 3, 2, 5, 1, 2, 2, 4, 4, 4, 3, 4, 4, 3, 3, 5, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 5, 4, 3, 3, 5, 3, 4, 4, 3, 5, 3, 5, 2, 5, 4, 2, 0, 3, 5, 3, 2, 3, 4, 0, 3, 4, 3, 5, 4, 3, 4, 2, 2, 2, 3, 5, 2, 2, 3, 0, 3, 3, 4, 3, 3, 3, 0, 4, 3, 3, 2, 5, 4, 3, 3, 3, 2, 5, 3, 2, 3, 4, 0, 3, 3, 3, 5, 4, 3, 4, 2, 5, 3, 0, 3, 5, 5, 0, 2, 3, 4, 0, 2, 5, 3, 3, 3, 0, 4, 3, 0, 0, 4, 4, 5, 5, 3, 5, 3, 5, 3, 0, 5, 4, 3, 3, 3, 3, 3, 2, 2, 3, 4, 5, 3, 4, 2, 3, 3, 0, 4, 3, 4, 3, 0, 3, 3, 2, 3, 0, 2, 4, 3, 5, 4, 2, 4, 3, 0, 4, 0, 3, 1, 3, 0, 4, 4, 1, 2, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 4, 4, 3, 3, 3]\n"
          ]
        }
      ]
    }
  ]
}