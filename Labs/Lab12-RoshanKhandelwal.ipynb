{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 1038.128881,
      "end_time": "2021-05-26T11:06:55.628764",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-05-26T10:49:37.499883",
      "version": "2.3.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "papermill": {
          "duration": 0.026881,
          "end_time": "2021-05-26T10:49:44.071762",
          "exception": false,
          "start_time": "2021-05-26T10:49:44.044881",
          "status": "completed"
        },
        "tags": [],
        "id": "irish-basket"
      },
      "source": [
        "**Lab 12 - Attention-based BiLSTM Classifier for Sentiment Analysis**\n",
        "\n",
        "Last week we applied different embedding techniques to the building of BiLSTM classifiers for sentiment analysis. In today's lab we add an Attention layer to the models."
      ],
      "id": "irish-basket"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-26T10:49:44.127999Z",
          "iopub.status.busy": "2021-05-26T10:49:44.126462Z",
          "iopub.status.idle": "2021-05-26T10:49:50.181944Z",
          "shell.execute_reply": "2021-05-26T10:49:50.181353Z"
        },
        "papermill": {
          "duration": 6.084801,
          "end_time": "2021-05-26T10:49:50.182101",
          "exception": false,
          "start_time": "2021-05-26T10:49:44.097300",
          "status": "completed"
        },
        "tags": [],
        "id": "metallic-award",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95e79155-dab3-4bd3-ba20-d98519d4e17c"
      },
      "source": [
        "import pandas as pd # To read data from CSVs directly into dataframes\n",
        "import numpy as np # For doing a data type cast\n",
        "from tensorflow import keras # As usual, we will use keras as a front end to TF\n",
        "from tqdm import tqdm # This is an iterator that makes nice prorgess bars\n",
        "import nltk # For doing some preprocessing on our string data\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords # We will do some cleanup of the text\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize # Cleanup will start with tokenization\n",
        "import re # We may use some regex in the cleanup\n",
        "\n",
        "\n",
        "nltk.download('punkt') # We need this parser - Colab does not have it by default\n",
        "\n",
        "# Utility functions from TF\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "# Some Keras utility functions and sequential layers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional, Dropout\n",
        "\n",
        "# For building our own Word2vec model\n",
        "from gensim.models import Word2Vec"
      ],
      "id": "metallic-award",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DecBW4tVnaGd"
      },
      "source": [
        "We will use some Yelp reviews that have been widely distributed and used as the basis of competitions at Kaggle. The data files are hosted on a dropbox account for convenience of downloading. If you would like to review a copy of this dataset, try: https://www.kaggle.com/ilhamfp31/yelp-review-dataset. If the dropbox download does not work, try getting the dataset from Kaggle and uploading them to the VM that is running your notebook. Note that a couple lines of code for loading the test data from the Kaggle contest have been commented out in the next (hidden) code block. There's so much data in the training set that we can just grab a radom sample of instances of that for validation."
      ],
      "id": "DecBW4tVnaGd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-WVe3ppPz2-"
      },
      "source": [
        "#@title\n",
        "# Retrieving the test data from drop box is commented out for now.\n",
        "# How might this test data be useful?\n",
        "\n",
        "#!wget https://www.dropbox.com/s/nnuxlff1dlgtjf0/YelpTest.csv?dl=1\n",
        "#test_data = 'YelpTest.csv?dl=1'"
      ],
      "id": "W-WVe3ppPz2-",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-c64Pc4QX3j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "851cbdcd-e1e4-47cd-f635-f4170d470410"
      },
      "source": [
        "# We are using the ! to access a terminal command. The colab image\n",
        "# comes with the wget utility which can download files from URLs.\n",
        "\n",
        "# This one is 400 MB, so it takes a moment\n",
        "!wget https://www.dropbox.com/s/70skwugmktk0idf/YelpTrain.csv?dl=1\n",
        "train_data = 'YelpTrain.csv?dl=1'\n"
      ],
      "id": "W-c64Pc4QX3j",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-21 18:39:56--  https://www.dropbox.com/s/70skwugmktk0idf/YelpTrain.csv?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/dl/70skwugmktk0idf/YelpTrain.csv [following]\n",
            "--2023-11-21 18:39:57--  https://www.dropbox.com/s/dl/70skwugmktk0idf/YelpTrain.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc1e30455df0bcd74447b34ccaf6.dl.dropboxusercontent.com/cd/0/get/CH8rqu6G9YAAk_LoW4jJ3MtKg3YfUVeOdEqJnnBuosb4yjjar_zd36nX-yLlTHCSZABJvSmctbzEXszFuvhiCNNl-fOIeBcoMYLcsRcuCf3udRMXOu0GquM3UDCxzxxbc14SPW1HWhWCRG2bnqSr45PY/file?dl=1# [following]\n",
            "--2023-11-21 18:39:57--  https://uc1e30455df0bcd74447b34ccaf6.dl.dropboxusercontent.com/cd/0/get/CH8rqu6G9YAAk_LoW4jJ3MtKg3YfUVeOdEqJnnBuosb4yjjar_zd36nX-yLlTHCSZABJvSmctbzEXszFuvhiCNNl-fOIeBcoMYLcsRcuCf3udRMXOu0GquM3UDCxzxxbc14SPW1HWhWCRG2bnqSr45PY/file?dl=1\n",
            "Resolving uc1e30455df0bcd74447b34ccaf6.dl.dropboxusercontent.com (uc1e30455df0bcd74447b34ccaf6.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to uc1e30455df0bcd74447b34ccaf6.dl.dropboxusercontent.com (uc1e30455df0bcd74447b34ccaf6.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 410758837 (392M) [application/binary]\n",
            "Saving to: ‘YelpTrain.csv?dl=1’\n",
            "\n",
            "YelpTrain.csv?dl=1  100%[===================>] 391.73M  72.2MB/s    in 4.6s    \n",
            "\n",
            "2023-11-21 18:40:02 (84.5 MB/s) - ‘YelpTrain.csv?dl=1’ saved [410758837/410758837]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxT_CrzBoK70"
      },
      "source": [
        "This dataset has already been divided into training and test sets probably because it was used in a Kaggle contest. In a content setting, analysts cannot generate their own train/test splits because different splits would not necessarily be comparable. For the sentiment variable, negative polarity is class 1, and positive class 2."
      ],
      "id": "cxT_CrzBoK70"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-26T10:49:50.385646Z",
          "iopub.status.busy": "2021-05-26T10:49:50.385093Z",
          "iopub.status.idle": "2021-05-26T10:49:58.992368Z",
          "shell.execute_reply": "2021-05-26T10:49:58.994383Z"
        },
        "papermill": {
          "duration": 8.661018,
          "end_time": "2021-05-26T10:49:58.994632",
          "exception": false,
          "start_time": "2021-05-26T10:49:50.333614",
          "status": "completed"
        },
        "tags": [],
        "id": "disabled-bachelor"
      },
      "source": [
        "# Pandas gives a simple way of reading a CSV directly into a dataframe.\n",
        "train = pd.read_csv(train_data, names = ['sentiment', 'text'] )\n",
        "\n",
        "# We don't need this unless we plan to use the test data.\n",
        "#test  = pd.read_csv(test_data,  names = ['sentiment', 'text'] )"
      ],
      "id": "disabled-bachelor",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-26T10:49:59.234263Z",
          "iopub.status.busy": "2021-05-26T10:49:59.233422Z",
          "iopub.status.idle": "2021-05-26T10:49:59.323003Z",
          "shell.execute_reply": "2021-05-26T10:49:59.323699Z"
        },
        "papermill": {
          "duration": 0.146697,
          "end_time": "2021-05-26T10:49:59.323904",
          "exception": false,
          "start_time": "2021-05-26T10:49:59.177207",
          "status": "completed"
        },
        "tags": [],
        "id": "neural-milton"
      },
      "source": [
        "training_size = 50000 # This cuts the data down to save some processing time\n",
        "# Once you have a preferred model, it would be a good idea to raise this to a\n",
        "# higher value such as 100,000 and retrain.\n",
        "\n",
        "train = train[:training_size]\n"
      ],
      "id": "neural-milton",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.082434,
          "end_time": "2021-05-26T10:49:59.473040",
          "exception": false,
          "start_time": "2021-05-26T10:49:59.390606",
          "status": "completed"
        },
        "tags": [],
        "id": "diagnostic-visit"
      },
      "source": [
        "A cleaning function can easily be applied across all of the texts in a pandas dataframe text variable. Let's define a custom function to pass to apply(). Remember that a function like this has to be interpreted each time it is caleld, so it is important to make it as efficient as possible."
      ],
      "id": "diagnostic-visit"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-26T10:49:59.656626Z",
          "iopub.status.busy": "2021-05-26T10:49:59.655822Z",
          "iopub.status.idle": "2021-05-26T10:49:59.672440Z",
          "shell.execute_reply": "2021-05-26T10:49:59.669375Z"
        },
        "papermill": {
          "duration": 0.111342,
          "end_time": "2021-05-26T10:49:59.672627",
          "exception": false,
          "start_time": "2021-05-26T10:49:59.561285",
          "status": "completed"
        },
        "tags": [],
        "id": "authentic-dispute"
      },
      "source": [
        "my_stops = stopwords.words('english')\n",
        "stop_pat = r'\\b(?:{})\\b'.format('|'.join(my_stops))\n",
        "\n",
        "def ReturnCleanText(text):\n",
        "         text = text.lower()\n",
        "         text = re.sub(r\"\\W+|_\", ' ', text)\n",
        "\n",
        "         return re.sub(stop_pat, '', text)\n",
        "\n",
        "# Now use the apply() method to run the function on each text\n",
        "train['clean_text'] = train['text'].apply(ReturnCleanText)\n"
      ],
      "id": "authentic-dispute",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.091219,
          "end_time": "2021-05-26T10:49:59.848022",
          "exception": false,
          "start_time": "2021-05-26T10:49:59.756803",
          "status": "completed"
        },
        "tags": [],
        "id": "applicable-variance"
      },
      "source": [
        "# Set up our first LSTM model:\n",
        "\n",
        "We are now going to pre-process our text and configure a LSTM model with a trainable embedding layer. This first block is just a brief experiment to test out the Leras TextVectorization. We use the results just to understand what the vectorization process looks like. Take a peek at https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization for more information on this approach."
      ],
      "id": "applicable-variance"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-26T10:50:00.069156Z",
          "iopub.status.busy": "2021-05-26T10:50:00.068337Z",
          "iopub.status.idle": "2021-05-26T10:50:03.391223Z",
          "shell.execute_reply": "2021-05-26T10:50:03.390433Z"
        },
        "papermill": {
          "duration": 3.3539,
          "end_time": "2021-05-26T10:50:03.391348",
          "exception": false,
          "start_time": "2021-05-26T10:50:00.037448",
          "status": "completed"
        },
        "tags": [],
        "id": "moderate-schema",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda1c405-7e59-40bd-c55c-8500591e5e94"
      },
      "source": [
        "# Let's do a test of the TextVectorization encoder\n",
        "max_features = 2000 # This is quite a limited vocabulary size\n",
        "\n",
        "# The vocabulary can have unlimited size or be capped, depending on the\n",
        "# class instantiation options for this layer; if there are more unique values\n",
        "# in the input than the maximum vocabulary size, the most frequent terms\n",
        "# will be used to create the vocabulary. This suggests that if the vocab will\n",
        "# be capped, we would need to normalize and do stop word removal.\n",
        "\n",
        "Encoder = keras.layers.experimental.preprocessing.TextVectorization( max_tokens = max_features)\n",
        "# By default this lowercases and strips punctuation\n",
        "\n",
        "Encoder.adapt(train['clean_text'].values) #\n",
        "\n",
        "vocab = np.array(Encoder.get_vocabulary())\n",
        "print(vocab[:20])\n",
        "\n",
        "example =\"Always a great example for showing fun results!\"\n",
        "print(Encoder(example).numpy())\n",
        "print(\" \".join(vocab[Encoder(example).numpy()]))"
      ],
      "id": "moderate-schema",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['' '[UNK]' 'n' 'food' 'place' 'good' 'like' 'get' 'one' 'time' 'would'\n",
            " 'service' 'back' 'great' 'go' 'really' 'even' 'ni' 'us' 'never']\n",
            "[  28    1   13 1749    1 1839  319 1844]\n",
            "always [UNK] great example [UNK] showing fun results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xG9YpyIZH1Pv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c10c6e55-de6a-43f4-9de2-b6685a16a79f"
      },
      "source": [
        "#\n",
        "# Exercise 11.5: Find out the length of the vocab array. Add a comment about\n",
        "# the size of the vocabulary. Is it large enough?\n",
        "print(len(vocab))\n",
        "#"
      ],
      "id": "xG9YpyIZH1Pv",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNUxMKQGwYf_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "cbaaa4f6-ffa3-4d91-e442-6132f3227eec"
      },
      "source": [
        "train['clean_text'].max() # Gives the max length string, 32 tokens\n",
        "# Useful as a guide to setting max_message below."
      ],
      "id": "LNUxMKQGwYf_",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'zumba tony brandon ryry   fantastic  recently moved back  phoenix  san diego  even though     zumba    years every class  different  guys  fun energetic motivating  great play lists  easy  follow even    new  zumba  love  classes '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-26T10:50:03.471787Z",
          "iopub.status.busy": "2021-05-26T10:50:03.461584Z",
          "iopub.status.idle": "2021-05-26T10:50:06.141945Z",
          "shell.execute_reply": "2021-05-26T10:50:06.141394Z"
        },
        "papermill": {
          "duration": 2.724528,
          "end_time": "2021-05-26T10:50:06.142084",
          "exception": false,
          "start_time": "2021-05-26T10:50:03.417556",
          "status": "completed"
        },
        "tags": [],
        "id": "pursuant-milan",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2581cc5e-8dd4-46cf-8d79-35b0219cb7d5"
      },
      "source": [
        "# Here's the real text pre-processing approach we are taking for this notebook.\n",
        "# We use the Keras Tokenizer to prepare our text.\n",
        "\n",
        "max_features = 2000 # As used in the next line, this only keeps the 2000 most frequent words\n",
        "max_message = 35 # A relatively short message\n",
        "\n",
        "tokenizer = Tokenizer(num_words = max_features, ) #https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
        "tokenizer.fit_on_texts(train['clean_text'].values)\n",
        "X = tokenizer.texts_to_sequences(train['clean_text'].values)\n",
        "\n",
        "# It is important to measure the longest message before setting this\n",
        "X = pad_sequences(X, padding = 'post' ,maxlen=max_message)\n",
        "Y = pd.get_dummies(train['sentiment']).values\n",
        "\n",
        "vocab_size = X.max() + 1 # We need to have one extra slot for the OOV/UNK token\n",
        "vocab_size # Show the vocab size: Should match max_features from above"
      ],
      "id": "pursuant-milan",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-26T10:50:06.201416Z",
          "iopub.status.busy": "2021-05-26T10:50:06.200301Z",
          "iopub.status.idle": "2021-05-26T10:50:06.209768Z",
          "shell.execute_reply": "2021-05-26T10:50:06.209147Z"
        },
        "papermill": {
          "duration": 0.040894,
          "end_time": "2021-05-26T10:50:06.209914",
          "exception": false,
          "start_time": "2021-05-26T10:50:06.169020",
          "status": "completed"
        },
        "tags": [],
        "id": "intermediate-subscription"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Not enturely clear why we are doing a train/test split on the training data\n",
        "# when we already have a test data set.\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.25, random_state = 42)\n"
      ],
      "id": "intermediate-subscription",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Xf2cyadIgGo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fce32d10-c2fb-4787-e0ab-8c8e3a4beb20"
      },
      "source": [
        "#\n",
        "# Exercise 11.6: Show the shape attributes for X_train, X_test, Y_train, Y_test\n",
        "print(\"X_train shape: \",X_train.shape)\n",
        "print(\"X_test shape: \",X_test.shape)\n",
        "print(\"Y_train shape: \",Y_train.shape)\n",
        "print(\"Y_test shape: \",Y_test.shape)\n",
        "#"
      ],
      "id": "0Xf2cyadIgGo",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape:  (37500, 35)\n",
            "X_test shape:  (12500, 35)\n",
            "Y_train shape:  (37500, 2)\n",
            "Y_test shape:  (12500, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.026001,
          "end_time": "2021-05-26T10:50:06.263970",
          "exception": false,
          "start_time": "2021-05-26T10:50:06.237969",
          "status": "completed"
        },
        "tags": [],
        "id": "sharp-johns"
      },
      "source": [
        "# Training with Keras default Embedding Layer"
      ],
      "id": "sharp-johns"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.025855,
          "end_time": "2021-05-26T10:50:06.316076",
          "exception": false,
          "start_time": "2021-05-26T10:50:06.290221",
          "status": "completed"
        },
        "tags": [],
        "id": "checked-reminder"
      },
      "source": [
        "### Keras Embedding Layer:\n",
        "\n",
        "Embedding layers in Keras are trained just like any other layer in a network architecture: Embedding weights will be trained using backprop to minimize the loss function by using the selected optimization method.\n",
        "\n",
        "Think about how this trained Embedding layer in Keras would be similar or different to a pretrained vector model like word2vec. By training the embeddings yourself, you minimize the loss function in an effort to accurately predict the two sentiments in the Yelp data. Will the learned embeddings capture complete word semantics?\n",
        "\n",
        "More Here:\n",
        "1. https://stats.stackexchange.com/questions/324992/how-the-embedding-layer-is-trained-in-keras-embedding-layer\n",
        "2. https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work"
      ],
      "id": "checked-reminder"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-26T10:50:06.375624Z",
          "iopub.status.busy": "2021-05-26T10:50:06.375129Z",
          "iopub.status.idle": "2021-05-26T10:50:06.864637Z",
          "shell.execute_reply": "2021-05-26T10:50:06.863960Z"
        },
        "papermill": {
          "duration": 0.522113,
          "end_time": "2021-05-26T10:50:06.864807",
          "exception": false,
          "start_time": "2021-05-26T10:50:06.342694",
          "status": "completed"
        },
        "tags": [],
        "id": "moral-causing",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28e657fd-2544-4377-9d5d-e70e4e0e9dfe"
      },
      "source": [
        "embid_dim = 100\n",
        "lstm_out = 64\n",
        "\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(Embedding(max_features, embid_dim, input_length = X.shape[1]))\n",
        "model.add(Bidirectional(LSTM(lstm_out, dropout=0.2)))\n",
        "model.add(Dense(128, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "# model.add(Dense(64, activation = 'relu'))\n",
        "model.add(Dense(2, activation = 'softmax')) # Why do we need this dense layer?\n",
        "model.summary()"
      ],
      "id": "moral-causing",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 35, 100)           200000    \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 128)               84480     \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 301250 (1.15 MB)\n",
            "Trainable params: 301250 (1.15 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-26T10:50:06.930846Z",
          "iopub.status.busy": "2021-05-26T10:50:06.929993Z",
          "iopub.status.idle": "2021-05-26T10:50:55.440418Z",
          "shell.execute_reply": "2021-05-26T10:50:55.439963Z"
        },
        "papermill": {
          "duration": 48.548299,
          "end_time": "2021-05-26T10:50:55.440581",
          "exception": false,
          "start_time": "2021-05-26T10:50:06.892282",
          "status": "completed"
        },
        "tags": [],
        "id": "seventh-operator",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48dab00d-f2fc-49b0-d620-f23440cd88da"
      },
      "source": [
        "batch_size = 128\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "history = model.fit(X_train, Y_train, epochs = 4, batch_size=batch_size, verbose = 1, validation_data =(X_test, Y_test))\n",
        "\n",
        "#\n",
        "# Take careful note of the training history. How is the validation accuracy\n",
        "# changing across epochs?\n",
        "#"
      ],
      "id": "seventh-operator",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "293/293 [==============================] - 61s 193ms/step - loss: 0.3533 - accuracy: 0.8368 - val_loss: 0.2876 - val_accuracy: 0.8752\n",
            "Epoch 2/4\n",
            "293/293 [==============================] - 48s 164ms/step - loss: 0.2734 - accuracy: 0.8862 - val_loss: 0.2904 - val_accuracy: 0.8753\n",
            "Epoch 3/4\n",
            "293/293 [==============================] - 49s 167ms/step - loss: 0.2500 - accuracy: 0.8955 - val_loss: 0.2836 - val_accuracy: 0.8799\n",
            "Epoch 4/4\n",
            "293/293 [==============================] - 48s 164ms/step - loss: 0.2214 - accuracy: 0.9070 - val_loss: 0.3017 - val_accuracy: 0.8753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.137436,
          "end_time": "2021-05-26T10:50:55.716655",
          "exception": false,
          "start_time": "2021-05-26T10:50:55.579219",
          "status": "completed"
        },
        "tags": [],
        "id": "romance-kennedy"
      },
      "source": [
        "# Training with GloVe 100D Embeddings\n",
        "\n",
        "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. For more info: https://nlp.stanford.edu/projects/glove/"
      ],
      "id": "romance-kennedy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m690N6djpnkw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16e6bcdb-2c18-4268-8eec-f8aecf3dab8c"
      },
      "source": [
        "embedding_width = 100\n",
        "\n",
        "# Now grab the GloVe embeddings we will need: Takes about a minute to download.\n",
        "# Then it take about another minute to fill the data structure.\n",
        "# Note that the zip file with the embeddings is hosted in Dropbox. If this does\n",
        "# not work, it could be downloaded from the GloVe website and uploaded to the\n",
        "# file store for the VM running this notebook.\n",
        "\n",
        "#!wget https://www.dropbox.com/s/ewfdwppopt3pild/glove.twitter.27B.100d.txt.zip?dl=1\n",
        "#!unzip glove.twitter.27B.100d.txt.zip?dl=1\n",
        "\n",
        "\n",
        "!wget https://www.dropbox.com/s/ewfdwppopt3pild/glove.twitter.27B.100d.txt.zip?dl=1\n",
        "!unzip glove.twitter.27B.100d.txt.zip?dl=1\n",
        "from google.colab import drive\n",
        "drive.mount('/drive')\n",
        "\n",
        "\n",
        "print(\"Loading word embeddings...\")\n",
        "embedding_vector = dict() # Initialize an empty dictionary\n",
        "embedding_dir = '/drive/My Drive/Colab Notebooks/IST 664/glove.twitter.27B.100d.txt'\n",
        "\n",
        "\n",
        "\n",
        "f = open(embedding_dir,encoding=\"utf8\")\n",
        "for line in f:\n",
        "    values = line.split() # Split the line on white space\n",
        "    word = values[0] # This is the word, so use it as the key\n",
        "    coefs = np.asarray(values[1:], dtype='float32') # Here are the values for each dimension of the vector for this word\n",
        "    embedding_vector[word] = coefs # Add to the dictionary\n",
        "f.close()\n",
        "\n",
        "print('Loaded %s word vectors.' % len(embedding_vector))\n"
      ],
      "id": "m690N6djpnkw",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-21 19:08:41--  https://www.dropbox.com/s/ewfdwppopt3pild/glove.twitter.27B.100d.txt.zip?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/dl/ewfdwppopt3pild/glove.twitter.27B.100d.txt.zip [following]\n",
            "--2023-11-21 19:08:42--  https://www.dropbox.com/s/dl/ewfdwppopt3pild/glove.twitter.27B.100d.txt.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2023-11-21 19:08:42 ERROR 404: Not Found.\n",
            "\n",
            "unzip:  cannot find or open glove.twitter.27B.100d.txt.zip?dl=1, glove.twitter.27B.100d.txt.zip?dl=1.zip or glove.twitter.27B.100d.txt.zip?dl=1.ZIP.\n",
            "\n",
            "No zipfiles found.\n",
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
            "Loading word embeddings...\n",
            "Loaded 1193514 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75295VSE4xwk"
      },
      "source": [
        "At this point we have all of the gloVe vectors for more than a million vocabularly words. In the code above, however, we have constrained the vocabulary we are using to train the embedding layer of our LSTM model. So if we are going to substitute these gloVe embedding weights instead of training our own weights, we need to grab just the weights we need for our vocab. We will use a little for loop to fill a weight structure that will later be passed into our model."
      ],
      "id": "75295VSE4xwk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-KMJAdu9LBV",
        "outputId": "a78c8b57-864a-434f-bcbf-4f2e7c86214d"
      },
      "source": [
        "# Fill our matrix value with zeroes. A vector of zeroes will be the default\n",
        "# if we don't match a token with the gloVe vocabulary.\n",
        "embedding_matrix = np.zeros((vocab_size,embedding_width))\n",
        "\n",
        "for i in range(1, (tokenizer.num_words-1)):\n",
        "\n",
        "  word = tokenizer.index_word[i] # This is the string value we want to use for the lookup\n",
        "  embedding_value = embedding_vector.get(word) # This does the dictionary lookup\n",
        "  if embedding_value is not None:\n",
        "    embedding_matrix[i] = embedding_value\n",
        "\n",
        "embedding_matrix.shape"
      ],
      "id": "c-KMJAdu9LBV",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now let's add an Attention layer, code adapted from https://tinyurl.com/attention664\n",
        "from keras import backend as K\n",
        "from keras import initializers\n",
        "\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        # Nothing special to be done here\n",
        "     #   self.init = initializers.get('glorot_uniform')\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Define the shape of the weights and bias in this layer\n",
        "        #\n",
        "\n",
        "# 12.1 Please explain here the shape size for the weights and for the bias\n",
        "# 128+35\n",
        "\n",
        "        self.w=self.add_weight(shape=(128,1), initializer= \"random_normal\", trainable = True)\n",
        "        self.b=self.add_weight(shape=(35,1), initializer=\"zero\", trainable = True)\n",
        "\n",
        "#12.2. Please modify the two lines of code above to obtain the shape size information from input_shape\n",
        "\n",
        "        super(Attention, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        # x is the input tensor (128 dimensions in our case here)\n",
        "        # Below is the main processing done during training\n",
        "        # K is the Keras Backend import\n",
        "         # Alignment scores. Pass them through tanh function\n",
        "        e = K.tanh(K.dot(x,self.w)+self.b)\n",
        "        # Remove dimension of size 1\n",
        "        e = K.squeeze(e, axis=-1)\n",
        "        # Compute the weights\n",
        "        alpha = K.softmax(e)\n",
        "        # Reshape to tensorFlow format\n",
        "        alpha = K.expand_dims(alpha, axis=-1)\n",
        "        # Compute the context vector\n",
        "        context = x * alpha\n",
        "        context = K.sum(context, axis=1)\n",
        "\n",
        "        return context\n",
        "\n"
      ],
      "metadata": {
        "id": "rmR9RhVw2RB-"
      },
      "id": "rmR9RhVw2RB-",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jG9BvRb6DYg"
      },
      "source": [
        "Now let's train a LSTM model using basically the same architecture as above, but in this case we will substitute our gloVe vectors into the embedding structure AND set the embedding layer to not be trainable. So whatever the vectors from gloVe happen to \"say\" about the meaning of a particular word, that's what we are sticking with for our model."
      ],
      "id": "7jG9BvRb6DYg"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-26T10:54:51.478069Z",
          "iopub.status.busy": "2021-05-26T10:54:51.477102Z",
          "iopub.status.idle": "2021-05-26T10:54:52.173009Z",
          "shell.execute_reply": "2021-05-26T10:54:52.173426Z"
        },
        "papermill": {
          "duration": 1.467787,
          "end_time": "2021-05-26T10:54:52.173634",
          "exception": false,
          "start_time": "2021-05-26T10:54:50.705847",
          "status": "completed"
        },
        "tags": [],
        "id": "worldwide-month",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e164bd1-3491-4766-99c7-b77618c00791"
      },
      "source": [
        "\n",
        "# lstm_out = 64 # Note that this value has been set above. Uncomment if you want\n",
        "# to use a different value.\n",
        "\n",
        "embed_dim = embedding_width # This needs to match the width of our gloVe vectors\n",
        "\n",
        "model = keras.Sequential()\n",
        "\n",
        "# The big difference here is that the embedding values are set to not trainable\n",
        "model.add(Embedding(vocab_size, embed_dim, input_length =X.shape[1], weights = [embedding_matrix] , trainable = False))\n",
        "model.add(Bidirectional(LSTM(lstm_out, dropout=0.2, return_sequences=True)))\n",
        "model.add(Attention())\n",
        "model.add(Dense(128, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "#model.add(Dense(64, activation = 'relu'))\n",
        "model.add(Dense(2, activation = 'softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "#12.3 Please leave comments here explaining the size of the parameters in the output table\n",
        "\n",
        "\"\"\"\n",
        "Embedding Layer (embedding_1):\n",
        "\n",
        "Output Shape: (None, 35, 100)\n",
        "Param  200,000\n",
        "\n",
        "Bidirectional LSTM Layer (bidirectional_1):\n",
        "\n",
        "Output Shape: (None, 35, 128)\n",
        "Param : 84,480\n",
        "\n",
        "Attention Layer (attention):\n",
        "\n",
        "Output Shape: (None, 128)\n",
        "Param : 163\n",
        "\n",
        "Dense Layer (dense_2):\n",
        "\n",
        "Output Shape: (None, 128)\n",
        "Param : 16,512\n",
        "\n",
        "Dropout Layer (dropout_1):\n",
        "\n",
        "Output Shape: (None, 128)\n",
        "Param : 0\n",
        "\n",
        "Dense Layer (dense_3):\n",
        "\n",
        "Output Shape: (None, 2)\n",
        "Param : 258\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "id": "worldwide-month",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 35, 100)           200000    \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 35, 128)           84480     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " attention (Attention)       (None, 128)               163       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 301413 (1.15 MB)\n",
            "Trainable params: 101413 (396.14 KB)\n",
            "Non-trainable params: 200000 (781.25 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-26T10:54:53.590093Z",
          "iopub.status.busy": "2021-05-26T10:54:53.589247Z",
          "iopub.status.idle": "2021-05-26T10:55:32.549654Z",
          "shell.execute_reply": "2021-05-26T10:55:32.549128Z"
        },
        "papermill": {
          "duration": 39.663126,
          "end_time": "2021-05-26T10:55:32.549800",
          "exception": false,
          "start_time": "2021-05-26T10:54:52.886674",
          "status": "completed"
        },
        "tags": [],
        "id": "absolute-programmer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a38a67f3-282b-4127-fbcc-191bc77fa3d3"
      },
      "source": [
        "batch_size = 64\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "history = model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 1, validation_data =(X_test, Y_test))"
      ],
      "id": "absolute-programmer",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "586/586 [==============================] - 73s 111ms/step - loss: 0.4064 - accuracy: 0.8148 - val_loss: 0.3428 - val_accuracy: 0.8518\n",
            "Epoch 2/5\n",
            "586/586 [==============================] - 66s 112ms/step - loss: 0.3429 - accuracy: 0.8498 - val_loss: 0.3395 - val_accuracy: 0.8505\n",
            "Epoch 3/5\n",
            "586/586 [==============================] - 58s 100ms/step - loss: 0.3194 - accuracy: 0.8616 - val_loss: 0.3036 - val_accuracy: 0.8667\n",
            "Epoch 4/5\n",
            "586/586 [==============================] - 58s 100ms/step - loss: 0.3004 - accuracy: 0.8724 - val_loss: 0.2971 - val_accuracy: 0.8725\n",
            "Epoch 5/5\n",
            "586/586 [==============================] - 64s 109ms/step - loss: 0.2867 - accuracy: 0.8783 - val_loss: 0.3078 - val_accuracy: 0.8675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxBsz4GcBuez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b374db77-b091-47ea-c865-0b3d9e5c1d22"
      },
      "source": [
        "# Now repeat that same model training, but allow the gloVe embedding weights\n",
        "# to be updated this time. What do you think may happen inside the model?\n",
        "\n",
        "model = keras.Sequential()\n",
        "\n",
        "model.add(Embedding(vocab_size, embid_dim, input_length =X.shape[1], weights = [embedding_matrix] , trainable = True))\n",
        "model.add(Bidirectional(LSTM(lstm_out, dropout=0.2)))\n",
        "model.add(Dense(128, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "#model.add(Dense(64, activation = 'relu'))\n",
        "model.add(Dense(2, activation = 'softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "history = model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 1, validation_data =(X_test, Y_test))"
      ],
      "id": "gxBsz4GcBuez",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 35, 100)           200000    \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 128)               84480     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 301250 (1.15 MB)\n",
            "Trainable params: 301250 (1.15 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "586/586 [==============================] - 75s 121ms/step - loss: 0.3702 - accuracy: 0.8349 - val_loss: 0.2856 - val_accuracy: 0.8779\n",
            "Epoch 2/5\n",
            "586/586 [==============================] - 69s 118ms/step - loss: 0.2810 - accuracy: 0.8815 - val_loss: 0.2689 - val_accuracy: 0.8877\n",
            "Epoch 3/5\n",
            "586/586 [==============================] - 71s 122ms/step - loss: 0.2551 - accuracy: 0.8937 - val_loss: 0.2775 - val_accuracy: 0.8851\n",
            "Epoch 4/5\n",
            "586/586 [==============================] - 76s 130ms/step - loss: 0.2405 - accuracy: 0.9005 - val_loss: 0.2763 - val_accuracy: 0.8875\n",
            "Epoch 5/5\n",
            "586/586 [==============================] - 69s 118ms/step - loss: 0.2198 - accuracy: 0.9090 - val_loss: 0.2942 - val_accuracy: 0.8858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWKJPFTlELN0"
      },
      "source": [
        "At this point you have examined three different models. Before moving on, review the training history for each model including the training time per epoch, the movement of the loss function in the training and validation samples, and the final validation accuracy. Given everything you have learned about deep learning for text processing, you should be able to describe in some detail what is happening inside these models and give a clear explanation of the trade-offs that are illustrated here."
      ],
      "id": "OWKJPFTlELN0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.86878,
          "end_time": "2021-05-26T10:55:34.380696",
          "exception": false,
          "start_time": "2021-05-26T10:55:33.511916",
          "status": "completed"
        },
        "tags": [],
        "id": "genetic-handbook"
      },
      "source": [
        "# Training with Gensim/Word2Vec Pre-trained and Trained Embeddings\n",
        "Reference: https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/"
      ],
      "id": "genetic-handbook"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.834104,
          "end_time": "2021-05-26T10:55:36.015903",
          "exception": false,
          "start_time": "2021-05-26T10:55:35.181799",
          "status": "completed"
        },
        "tags": [],
        "id": "challenging-trick"
      },
      "source": [
        "As you know from previous classes and labs **Word2Vec** is not one thing but rather a family of model architectures and optimizations that can be used to learn word embeddings from large datasets. In particular, there are two training variations that are important:\n",
        "\n",
        "1. Continuous Bag-of-Words Model which predicts the middle word based on surrounding context words. The context consists of a few words before and after the current (middle) word. This architecture is called a bag-of-words model as the order of words in the context is not important.\n",
        "\n",
        "2. Continuous Skip-gram Model which predict words within a certain range before and after the current word in the same sentence.\n",
        "\n",
        "As with other pre-trained embeddings, we have to choose the dimensionality of the vectors we will use. For consistency, we will use d=100 in the example below.\n",
        "\n",
        "More here:\n",
        "1. https://jalammar.github.io/illustrated-word2vec/\n",
        "2. https://www.tensorflow.org/tutorials/text/word2vec"
      ],
      "id": "challenging-trick"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-26T10:55:39.307650Z",
          "iopub.status.busy": "2021-05-26T10:55:39.307014Z",
          "iopub.status.idle": "2021-05-26T10:56:03.381008Z",
          "shell.execute_reply": "2021-05-26T10:56:03.381948Z"
        },
        "papermill": {
          "duration": 24.927891,
          "end_time": "2021-05-26T10:56:03.382144",
          "exception": false,
          "start_time": "2021-05-26T10:55:38.454253",
          "status": "completed"
        },
        "tags": [],
        "id": "arbitrary-intake",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8357ff0d-a1dd-4d91-c642-9c6687b8c128"
      },
      "source": [
        "# First, we will construct a long list out of all of the messages\n",
        "# in our dataset. This will serve as the basis for training a custom\n",
        "# word2vec model with Gensim.\n",
        "\n",
        "sentences =[]\n",
        "\n",
        "# tqdm is an iterator that provides an animated progress bar\n",
        "for t in  tqdm(range(len(train['clean_text']))):\n",
        "    text = nltk.word_tokenize(train['clean_text'][t])\n",
        "    sentences.append(text)"
      ],
      "id": "arbitrary-intake",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50000/50000 [00:25<00:00, 1941.98it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-26T10:56:07.136039Z",
          "iopub.status.busy": "2021-05-26T10:56:07.135361Z",
          "iopub.status.idle": "2021-05-26T10:56:07.279711Z",
          "shell.execute_reply": "2021-05-26T10:56:07.279163Z"
        },
        "papermill": {
          "duration": 1.005603,
          "end_time": "2021-05-26T10:56:07.280203",
          "exception": false,
          "start_time": "2021-05-26T10:56:06.274600",
          "status": "completed"
        },
        "tags": [],
        "id": "secure-textbook"
      },
      "source": [
        "# Takes about 1 minute for 50,000 sentences; For the sg= argument, use either 0 or 1. Default is 0 or CBOW. One must explicitly define Skip-gram by passing 1.\n",
        "\n",
        "w2v_model = Word2Vec(sentences, vector_size = 100, min_count=2, sg = 0)"
      ],
      "id": "secure-textbook",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-26T10:56:09.016965Z",
          "iopub.status.busy": "2021-05-26T10:56:09.016211Z",
          "iopub.status.idle": "2021-05-26T10:56:09.021381Z",
          "shell.execute_reply": "2021-05-26T10:56:09.020938Z"
        },
        "papermill": {
          "duration": 0.883679,
          "end_time": "2021-05-26T10:56:09.021493",
          "exception": false,
          "start_time": "2021-05-26T10:56:08.137814",
          "status": "completed"
        },
        "tags": [],
        "id": "minimal-stick",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f17916f-ea0c-4607-fee0-f64542c8969f"
      },
      "source": [
        "words = list(w2v_model.wv.index_to_key)\n",
        "print('Vocabulary size: %d' % len(words))\n",
        "\n",
        "# We can optionally save out model to a file, though this is not\n",
        "# need for the code below.\n",
        "\n",
        "#filename = 'embedding_word2vec.txt'\n",
        "#w2v_model.wv.save_word2vec_format(filename, binary=False)"
      ],
      "id": "minimal-stick",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 35010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7q2zuJZL_Ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "146c9d16-9a3c-4670-ce90-26fa5e61f38b"
      },
      "source": [
        "type(w2v_model)"
      ],
      "id": "W7q2zuJZL_Ae",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "gensim.models.word2vec.Word2Vec"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJbTCX-wN2hm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f94dc60-47ee-4f5e-b9bc-9719028c4769"
      },
      "source": [
        "# Just as we did for the gloVe model, we will now fill a custom embedding\n",
        "# matrix with weights from our custom word2vec model.\n",
        "embedding_matrix = np.zeros((vocab_size,embedding_width))\n",
        "oov_errors = 0\n",
        "\n",
        "for i in range(1, (tokenizer.num_words-1)):\n",
        "\n",
        "  word = tokenizer.index_word[i] # This is the string value we want to use for the lookup\n",
        "\n",
        "  # Here we need to catch KeyErrors so that we can proceed with the loop\n",
        "  # OOV words are simply left with weights of 0\n",
        "  try:\n",
        "    embedding_matrix[i] = w2v_model.wv[word]\n",
        "  except KeyError:\n",
        "    oov_errors += 1\n",
        "\n",
        "\n",
        "print(\"Out of vocab errors: \", oov_errors)"
      ],
      "id": "NJbTCX-wN2hm",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out of vocab errors:  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-26T10:56:15.585655Z",
          "iopub.status.busy": "2021-05-26T10:56:15.584746Z",
          "iopub.status.idle": "2021-05-26T10:56:16.216180Z",
          "shell.execute_reply": "2021-05-26T10:56:16.215622Z"
        },
        "papermill": {
          "duration": 1.498523,
          "end_time": "2021-05-26T10:56:16.216311",
          "exception": false,
          "start_time": "2021-05-26T10:56:14.717788",
          "status": "completed"
        },
        "tags": [],
        "id": "common-tender",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e99a70db-14a4-415b-c251-a6aa8ebcdda7"
      },
      "source": [
        "# Now, just as previously, create a basic LSTM model where the embedding\n",
        "# layer is populated with the weights from our custom word2vec model.\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(Embedding(vocab_size, embid_dim, input_length =X.shape[1], weights = [ embedding_matrix] , trainable = False))\n",
        "model.add(Bidirectional(LSTM(lstm_out, dropout=0.2)))\n",
        "model.add(Dense(128, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "#model.add(Dense(64, activation = 'relu'))\n",
        "model.add(Dense(2, activation = 'softmax'))\n",
        "model.summary()"
      ],
      "id": "common-tender",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 35, 100)           200000    \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirecti  (None, 128)               84480     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 301250 (1.15 MB)\n",
            "Trainable params: 101250 (395.51 KB)\n",
            "Non-trainable params: 200000 (781.25 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UFaZSJMA7wE"
      },
      "source": [
        "As before, it is important to look at the details of this model configuration and compare to the previous models you have created in this notebook. In particular, make sure to compare the overall model size and the number of trainable parameters."
      ],
      "id": "5UFaZSJMA7wE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-26T10:56:17.947848Z",
          "iopub.status.busy": "2021-05-26T10:56:17.946985Z",
          "iopub.status.idle": "2021-05-26T11:02:13.268074Z",
          "shell.execute_reply": "2021-05-26T11:02:13.267641Z"
        },
        "papermill": {
          "duration": 356.192194,
          "end_time": "2021-05-26T11:02:13.268197",
          "exception": false,
          "start_time": "2021-05-26T10:56:17.076003",
          "status": "completed"
        },
        "tags": [],
        "id": "damaged-english",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08fcc542-f4cc-44f5-ce60-9c2b9461d22f"
      },
      "source": [
        "batch_size = 128\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "history = model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 1, validation_data =(X_test, Y_test))"
      ],
      "id": "damaged-english",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "293/293 [==============================] - 51s 158ms/step - loss: 0.3700 - accuracy: 0.8354 - val_loss: 0.3057 - val_accuracy: 0.8694\n",
            "Epoch 2/5\n",
            "293/293 [==============================] - 43s 146ms/step - loss: 0.3031 - accuracy: 0.8712 - val_loss: 0.2873 - val_accuracy: 0.8756\n",
            "Epoch 3/5\n",
            "293/293 [==============================] - 49s 167ms/step - loss: 0.2837 - accuracy: 0.8785 - val_loss: 0.2954 - val_accuracy: 0.8767\n",
            "Epoch 4/5\n",
            "293/293 [==============================] - 43s 148ms/step - loss: 0.2697 - accuracy: 0.8877 - val_loss: 0.2844 - val_accuracy: 0.8803\n",
            "Epoch 5/5\n",
            "293/293 [==============================] - 50s 169ms/step - loss: 0.2547 - accuracy: 0.8934 - val_loss: 0.2823 - val_accuracy: 0.8836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkKbOUdWCmAS"
      },
      "source": [
        "Now give some thought to these custom word2vec vectors. In particular, think about generalizability of this model. How do you think this model would generalize to messages with different vocabulary?"
      ],
      "id": "rkKbOUdWCmAS"
    },
    {
      "cell_type": "code",
      "source": [
        "#Whether the model can understand and work well with different words in messages\n",
        "#depends on how good the custom word representations are, how well they understand the meanings of words,\n",
        "#and how flexible the model is. To make it even better at understanding various words,\n",
        "# we can use techniques like fine-tuning, adjusting data, and applying special methods during training."
      ],
      "metadata": {
        "id": "w4VYBR2iYR7Q"
      },
      "id": "w4VYBR2iYR7Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NjzvB2MIYRnk"
      },
      "id": "NjzvB2MIYRnk"
    }
  ]
}